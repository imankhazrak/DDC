{"cells":[{"cell_type":"markdown","metadata":{"id":"BOlo4Ctmtm4p"},"source":["# Deep transfer learning tutorial\n","This notebook contains two popular paradigms of transfer learning: **Finetune** and **Domain adaptation**.\n","Since most of the codes are shared by them, we show how they work in just one single notebook.\n","I think that transfer learning and domain adaptation are both easy, and there's no need to create some library or packages for this simple purpose, which only makes things difficult.\n","The purpose of this note book is we **don't even need to install a library or package** to train a domain adaptation or finetune model."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1143,"status":"ok","timestamp":1698802433798,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"agDvxay5xJLe","outputId":"7051928e-c8c1-4131-861d-4a922abab138"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"yyP-9VnQtm4t"},"source":["## Some imports."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2788,"status":"ok","timestamp":1698802436584,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"6R098oQTS_TC"},"outputs":[],"source":["import os\n","import pandas as pd\n","import torch\n","import torchvision\n","from torchvision import transforms, datasets\n","import torch.nn as nn\n","import time\n","from torchvision import models\n","torch.cuda.set_device(0)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1698802436585,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"YE8_6pxY2yIV"},"outputs":[],"source":["# !pip install torch\n","# !pip install torchvision\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1698802436585,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"rtQ8PiAJxHLN","outputId":"8457e17d-8596-4768-bda3-553457e1a9ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}],"source":["print(torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{"id":"K9Ut17hwUDq-"},"source":["Set the dataset folder, batch size, number of classes, and domain name."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698802436585,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"j4e--IIRU68M"},"outputs":[],"source":["data_folder = '/content/drive/MyDrive/4. BGSU/1.2 PhD_Courses/Z. RA & Desertation/Codes/Dr_Niu/MMD/Office31'\n","# data_folder = '/content/drive/MyDrive/3. Narges/Dr_Niu/MMD/Office31'\n","\n","batch_size = 32\n","n_class = 31\n","domain_src, domain_tar = 'amazon/images', 'dslr/images'"]},{"cell_type":"markdown","metadata":{"id":"XA6e2YaPtm4u"},"source":["## Data load\n","Now, define a data loader function."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698802436585,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"6JtN_bK9VFcM"},"outputs":[],"source":["def load_data(root_path, domain, batch_size, phase):\n","    transform_dict = {\n","        'src': transforms.Compose(\n","        [transforms.RandomResizedCrop(224),\n","         transforms.RandomHorizontalFlip(),\n","         transforms.ToTensor(),\n","         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                              std=[0.229, 0.224, 0.225]),\n","         ]),\n","        'tar': transforms.Compose(\n","        [transforms.Resize(224),\n","         transforms.ToTensor(),\n","         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                              std=[0.229, 0.224, 0.225]),\n","         ])}\n","    data = datasets.ImageFolder(root=os.path.join(root_path, domain), transform=transform_dict[phase])\n","    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=phase=='src', drop_last=phase=='tar', num_workers=4)\n","    return data_loader"]},{"cell_type":"markdown","metadata":{"id":"PHy09lD9tm4v"},"source":["Load the data using the above function to test it."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698802436585,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"Jf_Gw2HRVJM_","outputId":"3592e2c6-1b97-4c9b-c231-ecd25d73cfed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Source data number: 2817\n","Target data number: 498\n"]}],"source":["src_loader = load_data(data_folder, domain_src, batch_size, phase='src')\n","tar_loader = load_data(data_folder, domain_tar, batch_size, phase='tar')\n","print(f'Source data number: {len(src_loader.dataset)}')\n","print(f'Target data number: {len(tar_loader.dataset)}')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698802436585,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"OXAjmY7pVK8t"},"outputs":[],"source":["class TransferModel(nn.Module):\n","    def __init__(self,\n","                base_model : str = 'resnet101',\n","                pretrain : bool = True,\n","                n_class : int = 31):\n","        super(TransferModel, self).__init__()\n","        self.base_model = base_model\n","        self.pretrain = pretrain\n","        self.n_class = n_class\n","        if self.base_model == 'resnet101':\n","            self.model = torchvision.models.resnet101(pretrained=True)\n","            n_features = self.model.fc.in_features\n","            fc = torch.nn.Linear(n_features, n_class)\n","            self.model.fc = fc\n","        else:\n","            # Use other models you like, such as vgg or alexnet\n","            pass\n","        self.model.fc.weight.data.normal_(0, 0.005)\n","        self.model.fc.bias.data.fill_(0.1)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def predict(self, x):\n","        return self.forward(x)"]},{"cell_type":"markdown","metadata":{"id":"UAqT-0jRtm4w"},"source":["Now, we define a model and test it using a random tensor."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1841,"status":"ok","timestamp":1698802438422,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"LewRmYIvXEIo","outputId":"25171896-461e-494f-cfcc-6dfdd0d0d786"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[-0.0624,  0.0224,  0.0663,  0.0546,  0.2049,  0.0491,  0.2913, -0.1083,\n","          0.2125,  0.0620,  0.0358,  0.0655,  0.1039,  0.2989,  0.2533,  0.1460,\n","         -0.0006,  0.0731,  0.2567,  0.0580,  0.3471,  0.0365, -0.0559,  0.1622,\n","          0.1770,  0.0598,  0.0696,  0.2589,  0.0985,  0.1981,  0.1653]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","torch.Size([1, 31])\n"]}],"source":["model = TransferModel().cuda()\n","RAND_TENSOR = torch.randn(1, 3, 224, 224).cuda()\n","output = model(RAND_TENSOR)\n","print(output)\n","print(output.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698802438423,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"h74gKIVqtm4w"},"outputs":[],"source":["dataloaders = {'src': src_loader,\n","               'val': tar_loader,\n","               'tar': tar_loader}\n","n_epoch = 100\n","criterion = nn.CrossEntropyLoss()\n","early_stop = 20"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1698802438423,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"GR5Y1x4btm4y"},"outputs":[],"source":["def test(model, target_test_loader):\n","    model.eval()\n","    correct = 0\n","    len_target_dataset = len(target_test_loader.dataset)\n","    with torch.no_grad():\n","        for data, target in target_test_loader:\n","            data, target = data.cuda(), target.cuda()\n","            s_output = model.predict(data)\n","            pred = torch.max(s_output, 1)[1]\n","            correct += torch.sum(pred == target)\n","    acc = correct.double() / len(target_test_loader.dataset)\n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"bO4c_QcGtm4z"},"source":["## Domain adaptation\n","Now we are in domain adaptation."]},{"cell_type":"markdown","metadata":{"id":"VJwcwLQftm40"},"source":["## Logic for domain adaptation\n","The logic for domain adaptation is mostly similar to finetune, except that we must add a loss to the finetune model to **regularize the distribution discrepancy** between two domains.\n","Therefore, the most different parts are:\n","- Define some **loss function** to compute the distance (which is the main contribution of most existing DA papers)\n","- Define a new model class to use that loss function for **forward** pass.\n","- Write a slightly different script to train, since we have to take both **source data, source label, and target data**."]},{"cell_type":"markdown","metadata":{"id":"Jy_1xwdJtm40"},"source":["### Loss function\n","The most popular loss function for DA is **MMD (Maximum Mean Discrepancy)**. For comaprison, we also use another popular loss **CORAL (CORrelation ALignment)**. They are defined as follows."]},{"cell_type":"markdown","metadata":{"id":"z3-wKorUtm40"},"source":["#### MMD loss"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1698802438423,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"MpQH6VFwtm41"},"outputs":[],"source":["class MMD_loss(nn.Module):\n","    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5):\n","        super(MMD_loss, self).__init__()\n","        self.kernel_num = kernel_num\n","        self.kernel_mul = kernel_mul\n","        self.fix_sigma = None\n","        self.kernel_type = kernel_type\n","\n","    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n","        n_samples = int(source.size()[0]) + int(target.size()[0])\n","        total = torch.cat([source, target], dim=0)\n","        total0 = total.unsqueeze(0).expand(\n","            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n","        total1 = total.unsqueeze(1).expand(\n","            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n","        L2_distance = ((total0-total1)**2).sum(2)\n","        if fix_sigma:\n","            bandwidth = fix_sigma\n","        else:\n","            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n","        bandwidth /= kernel_mul ** (kernel_num // 2)\n","        bandwidth_list = [bandwidth * (kernel_mul**i)\n","                          for i in range(kernel_num)]\n","        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n","                      for bandwidth_temp in bandwidth_list]\n","        return sum(kernel_val)\n","\n","    def linear_mmd2(self, f_of_X, f_of_Y):\n","        loss = 0.0\n","        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n","        loss = delta.dot(delta.T)\n","        return loss\n","\n","    def forward(self, source, target):\n","        if self.kernel_type == 'linear':\n","            return self.linear_mmd2(source, target)\n","        elif self.kernel_type == 'rbf':\n","            batch_size = int(source.size()[0])\n","            kernels = self.guassian_kernel(\n","                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n","            XX = torch.mean(kernels[:batch_size, :batch_size])\n","            YY = torch.mean(kernels[batch_size:, batch_size:])\n","            XY = torch.mean(kernels[:batch_size, batch_size:])\n","            YX = torch.mean(kernels[batch_size:, :batch_size])\n","            loss = torch.mean(XX + YY - XY - YX)\n","            return loss\n"]},{"cell_type":"markdown","metadata":{"id":"NcfUy_2Dtm41"},"source":["#### CORAL loss"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698802438423,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"uZhKJq15tm41"},"outputs":[],"source":["def CORAL(source, target):\n","    d = source.size(1)\n","    ns, nt = source.size(0), target.size(0)\n","\n","    # source covariance\n","    tmp_s = torch.ones((1, ns)).cuda() @ source\n","    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)\n","\n","    # target covariance\n","    tmp_t = torch.ones((1, nt)).cuda() @ target\n","    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)\n","\n","    # frobenius norm\n","    loss = (cs - ct).pow(2).sum().sqrt()\n","    loss = loss / (4 * d * d)\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"VB2cDp8Gtm41"},"source":["### Model\n","Now we use ResNet-50 again just like finetune. The difference is that we rewrite the ResNet-50 class to drop its last layer."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698802438423,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"UOLx_OSxtm41"},"outputs":[],"source":["from torchvision import models\n","class resnet101Fc(nn.Module):\n","    def __init__(self):\n","        super(resnet101Fc, self).__init__()\n","        model_resnet101 = models.resnet101(pretrained=True)\n","        self.conv1 = model_resnet101.conv1\n","        self.bn1 = model_resnet101.bn1\n","        self.relu = model_resnet101.relu\n","        self.maxpool = model_resnet101.maxpool\n","        self.layer1 = model_resnet101.layer1\n","        self.layer2 = model_resnet101.layer2\n","        self.layer3 = model_resnet101.layer3\n","        self.layer4 = model_resnet101.layer4\n","        self.avgpool = model_resnet101.avgpool\n","        self.__in_features = model_resnet101.fc.in_features\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","    def output_num(self):\n","        return self.__in_features"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698802438423,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"ksoGpywfxHLS"},"outputs":[],"source":["# Function to visualize the convolutional kernels\n","def visualize_kernels(model):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Conv2d):\n","            kernels = module.weight.data.cpu().numpy()\n","            num_kernels = kernels.shape[0]\n","            fig, axs = plt.subplots(1, num_kernels, figsize=(10, 2))\n","            for i in range(num_kernels):\n","                kernel = kernels[i, 0, :, :]\n","                axs[i].imshow(kernel, cmap='gray')\n","                axs[i].axis('off')\n","            plt.show()\n","\n","# Function to compute the entropy of convolutional layers\n","def compute_entropy(model):\n","    entropy_values = []\n","    for module in model.modules():\n","        if isinstance(module, nn.Conv2d):\n","            weights = module.weight.data\n","            flattened_weights = weights.flatten()\n","#             normalized_weights = torch.softmax(flattened_weights, dim=1)\n","#             entropy = -torch.sum(normalized_weights * torch.log2(normalized_weights + 1e-10), dim=1)\n","            normalized_weights = (flattened_weights - torch.min(flattened_weights)) / (\n","                torch.max(flattened_weights) - torch.min(flattened_weights)\n","            )\n","            bin_counts = torch.histc(normalized_weights, bins=10)\n","            probabilities = bin_counts / len(flattened_weights)\n","            entropy = -torch.sum(probabilities * torch.log2(probabilities + 1e-10))  # Add a small constant to avoid log(0)\n","            entropy_values.append(torch.mean(entropy).item())\n","    return entropy_values\n","\n","# Function to add noise to an image\n","def add_noise(image, noise_level):\n","    noise = torch.randn_like(image) * noise_level\n","    noisy_image = image + noise\n","    noisy_image = torch.clamp(noisy_image, 0.0, 1.0)\n","    return noisy_image"]},{"cell_type":"markdown","metadata":{"id":"IRgdNM3Wtm42"},"source":["Now the main class for DA. We take ResNet-50 as its backbone, add a bottleneck layer and our own FC layer for classification.\n","Note the `adapt_loss` function. It is just using our predefined MMD or CORAL loss. Of course you can use your own loss."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1698802438424,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"oC5NKJpJtm42"},"outputs":[],"source":["class TransferNet(nn.Module):\n","    def __init__(self,\n","                 num_class,\n","                 base_net='resnet101',\n","                 transfer_loss='mmd',\n","                 use_bottleneck=True,\n","                 bottleneck_width=256,\n","                 width=1024):\n","        super(TransferNet, self).__init__()\n","        if base_net == 'resnet101':\n","            self.base_network = resnet101Fc()\n","        else:\n","            # Your own basenet\n","            return\n","        self.use_bottleneck = use_bottleneck\n","        self.transfer_loss = transfer_loss\n","        bottleneck_list = [nn.Linear(self.base_network.output_num(\n","        ), bottleneck_width), nn.BatchNorm1d(bottleneck_width), nn.ReLU(), nn.Dropout(0.5)]\n","        self.bottleneck_layer = nn.Sequential(*bottleneck_list)\n","        classifier_layer_list = [nn.Linear(self.base_network.output_num(), width), nn.ReLU(), nn.Dropout(0.5),\n","                                 nn.Linear(width, num_class)]\n","        self.classifier_layer = nn.Sequential(*classifier_layer_list)\n","\n","        self.bottleneck_layer[0].weight.data.normal_(0, 0.005)\n","        self.bottleneck_layer[0].bias.data.fill_(0.1)\n","        for i in range(2):\n","            self.classifier_layer[i * 3].weight.data.normal_(0, 0.01)\n","            self.classifier_layer[i * 3].bias.data.fill_(0.0)\n","\n","    def forward(self, source, target):\n","        source = self.base_network(source)\n","        target = self.base_network(target)\n","        source_clf = self.classifier_layer(source)\n","        if self.use_bottleneck:\n","            source = self.bottleneck_layer(source)\n","            target = self.bottleneck_layer(target)\n","        transfer_loss = self.adapt_loss(source, target, self.transfer_loss)\n","        return source_clf, transfer_loss\n","\n","    def predict(self, x):\n","        features = self.base_network(x)\n","        clf = self.classifier_layer(features)\n","        return clf\n","\n","    def adapt_loss(self, X, Y, adapt_loss):\n","        \"\"\"Compute adaptation loss, currently we support mmd and coral\n","\n","        Arguments:\n","            X {tensor} -- source matrix\n","            Y {tensor} -- target matrix\n","            adapt_loss {string} -- loss type, 'mmd' or 'coral'. You can add your own loss\n","\n","        Returns:\n","            [tensor] -- adaptation loss tensor\n","        \"\"\"\n","        if adapt_loss == 'mmd':\n","            mmd_loss = MMD_loss()\n","            loss = mmd_loss(X, Y)\n","        elif adapt_loss == 'coral':\n","            loss = CORAL(X, Y)\n","        else:\n","            # Your own loss\n","            loss = 0\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"hAdPs27btm42"},"source":["### Train\n","Now the train part."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1132,"status":"ok","timestamp":1698802439551,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"OK6P8uMDtm42"},"outputs":[],"source":["transfer_loss = 'mmd'\n","learning_rate = 0.0001\n","transfer_model = TransferNet(n_class, transfer_loss=transfer_loss, base_net='resnet101').cuda()\n","optimizer = torch.optim.SGD([\n","    {'params': transfer_model.base_network.parameters()},\n","    {'params': transfer_model.bottleneck_layer.parameters(), 'lr': 10 * learning_rate},\n","    {'params': transfer_model.classifier_layer.parameters(), 'lr': 10 * learning_rate},\n","], lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n","# lamb = 0.5 # weight for transfer loss, it is a hyperparameter that needs to be tuned"]},{"cell_type":"markdown","metadata":{"id":"4WpUfcHItm42"},"source":["The main train function. Since we have to enumerate all source and target samples, we have to use `zip` operation to enumerate each pair of these two domains. It is common that two domains have different sizes, but we think by randomly sampling them in many epochs, we may sample each one of them."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1698802439552,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"AGlVNI2ktm42"},"outputs":[],"source":["def train(dataloaders, model, optimizer):\n","    source_loader, target_train_loader, target_test_loader = dataloaders['src'], dataloaders['val'], dataloaders['tar']\n","    len_source_loader = len(source_loader)\n","    len_target_loader = len(target_train_loader)\n","    best_acc = 0\n","    stop = 0\n","    n_batch = min(len_source_loader, len_target_loader)\n","    for e in range(n_epoch):\n","        stop += 1\n","        train_loss_clf, train_loss_transfer, train_loss_total = 0, 0, 0\n","        model.train()\n","        for (src, tar) in zip(source_loader, target_train_loader):\n","            data_source, label_source = src\n","            data_target, _ = tar\n","            data_source, label_source = data_source.cuda(), label_source.cuda()\n","            data_target = data_target.cuda()\n","\n","            optimizer.zero_grad()\n","            label_source_pred, transfer_loss = model(data_source, data_target)\n","            clf_loss = criterion(label_source_pred, label_source)\n","            loss = clf_loss + lamb * transfer_loss\n","            loss.backward()\n","            optimizer.step()\n","            train_loss_clf = clf_loss.detach().item() + train_loss_clf\n","            train_loss_transfer = transfer_loss.detach().item() + train_loss_transfer\n","            train_loss_total = loss.detach().item() + train_loss_total\n","        acc = test(model, target_test_loader)\n","        print(f'Epoch: [{e:2d}/{n_epoch}], cls_loss: {train_loss_clf/n_batch:.4f}, transfer_loss: {train_loss_transfer/n_batch:.4f}, total_Loss: {train_loss_total/n_batch:.4f}, acc: {acc:.4f}')\n","\n","        if best_acc < acc:\n","            best_acc = acc\n","            torch.save(model.state_dict(), 'trans_model_webcam.pkl')\n","            stop = 0\n","        if stop >= early_stop:\n","            break"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698802439553,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"nqSCG6-Xtm43","scrolled":true},"outputs":[],"source":["# train(dataloaders, transfer_model, optimizer)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1698802439553,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"yWvYODRDtm43"},"outputs":[],"source":["# transfer_model.load_state_dict(torch.load('trans_model_webcam.pkl'))\n","# acc_test = test(transfer_model, dataloaders['tar'])\n","# print(f'Test accuracy: {acc_test}')"]},{"cell_type":"markdown","metadata":{"id":"jbw6KndNtm43"},"source":["Now we are done."]},{"cell_type":"markdown","metadata":{"id":"JQX2ak-jtm43"},"source":["You see, we don't even need to install a library or package to train a domain adaptation or finetune model.\n","In your own work, you can also use this notebook to test your own algorithms."]},{"cell_type":"markdown","metadata":{"id":"pvqYPfrMxHLU"},"source":["Add entropy"]},{"cell_type":"markdown","metadata":{"id":"vc6-Zp1wCE_-"},"source":["# Archive\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1698802439554,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"rbo4SFU8b4rS"},"outputs":[],"source":["def test2(model, target_test_loader, lamb):\n","    records = [] # new added\n","    model.eval()\n","    correct = 0\n","    len_target_dataset = len(target_test_loader.dataset)\n","    with torch.no_grad():\n","        for data, target in target_test_loader:\n","            data, target = data.cuda(), target.cuda()\n","            s_output = model.predict(data)\n","            pred = torch.max(s_output, 1)[1]\n","            correct += torch.sum(pred == target)\n","    acc = correct.double() / len(target_test_loader.dataset)\n","    records.append({ # new added\n","            'Lambda': lamb,\n","            'Test accuracy': acc,\n","    })\n","    global test_result\n","    test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","    return acc"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8304589,"status":"ok","timestamp":1698810744131,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"GVFOH_0ADch3","outputId":"6ef32be6-b742-4b9a-df40-f655d81a3b69"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.0 Epoch: [ 0/100], cls_loss: 3.4267, transfer_loss: 0.1635, total_Loss: 3.4267, acc: 0.0341\n","lambda= 0.0 Epoch: [ 1/100], cls_loss: 3.4061, transfer_loss: 0.1630, total_Loss: 3.4061, acc: 0.0422\n","lambda= 0.0 Epoch: [ 2/100], cls_loss: 3.3666, transfer_loss: 0.1642, total_Loss: 3.3666, acc: 0.0884\n","lambda= 0.0 Epoch: [ 3/100], cls_loss: 3.3476, transfer_loss: 0.1621, total_Loss: 3.3476, acc: 0.1928\n","lambda= 0.0 Epoch: [ 4/100], cls_loss: 3.3145, transfer_loss: 0.1634, total_Loss: 3.3145, acc: 0.1968\n","lambda= 0.0 Epoch: [ 5/100], cls_loss: 3.2898, transfer_loss: 0.1619, total_Loss: 3.2898, acc: 0.2972\n","lambda= 0.0 Epoch: [ 6/100], cls_loss: 3.2318, transfer_loss: 0.1641, total_Loss: 3.2318, acc: 0.2189\n","lambda= 0.0 Epoch: [ 7/100], cls_loss: 3.2256, transfer_loss: 0.1621, total_Loss: 3.2256, acc: 0.3193\n","lambda= 0.0 Epoch: [ 8/100], cls_loss: 3.1670, transfer_loss: 0.1628, total_Loss: 3.1670, acc: 0.4398\n","lambda= 0.0 Epoch: [ 9/100], cls_loss: 3.1073, transfer_loss: 0.1637, total_Loss: 3.1073, acc: 0.4157\n","lambda= 0.0 Epoch: [10/100], cls_loss: 3.0672, transfer_loss: 0.1640, total_Loss: 3.0672, acc: 0.3655\n","lambda= 0.0 Epoch: [11/100], cls_loss: 3.0089, transfer_loss: 0.1609, total_Loss: 3.0089, acc: 0.3755\n","lambda= 0.0 Epoch: [12/100], cls_loss: 2.9256, transfer_loss: 0.1626, total_Loss: 2.9256, acc: 0.4036\n","lambda= 0.0 Epoch: [13/100], cls_loss: 2.8899, transfer_loss: 0.1648, total_Loss: 2.8899, acc: 0.4578\n","lambda= 0.0 Epoch: [14/100], cls_loss: 2.8200, transfer_loss: 0.1639, total_Loss: 2.8200, acc: 0.5161\n","lambda= 0.0 Epoch: [15/100], cls_loss: 2.7015, transfer_loss: 0.1629, total_Loss: 2.7015, acc: 0.5120\n","lambda= 0.0 Epoch: [16/100], cls_loss: 2.6053, transfer_loss: 0.1643, total_Loss: 2.6053, acc: 0.4940\n","lambda= 0.0 Epoch: [17/100], cls_loss: 2.5797, transfer_loss: 0.1639, total_Loss: 2.5797, acc: 0.5020\n","lambda= 0.0 Epoch: [18/100], cls_loss: 2.4667, transfer_loss: 0.1643, total_Loss: 2.4667, acc: 0.5643\n","lambda= 0.0 Epoch: [19/100], cls_loss: 2.3952, transfer_loss: 0.1635, total_Loss: 2.3952, acc: 0.5984\n","lambda= 0.0 Epoch: [20/100], cls_loss: 2.3377, transfer_loss: 0.1640, total_Loss: 2.3377, acc: 0.5964\n","lambda= 0.0 Epoch: [21/100], cls_loss: 2.1858, transfer_loss: 0.1627, total_Loss: 2.1858, acc: 0.5904\n","lambda= 0.0 Epoch: [22/100], cls_loss: 2.1128, transfer_loss: 0.1642, total_Loss: 2.1128, acc: 0.5763\n","lambda= 0.0 Epoch: [23/100], cls_loss: 2.0194, transfer_loss: 0.1657, total_Loss: 2.0194, acc: 0.5703\n","lambda= 0.0 Epoch: [24/100], cls_loss: 1.9002, transfer_loss: 0.1635, total_Loss: 1.9002, acc: 0.6064\n","lambda= 0.0 Epoch: [25/100], cls_loss: 1.8971, transfer_loss: 0.1624, total_Loss: 1.8971, acc: 0.6084\n","lambda= 0.0 Epoch: [26/100], cls_loss: 1.8053, transfer_loss: 0.1639, total_Loss: 1.8053, acc: 0.6627\n","lambda= 0.0 Epoch: [27/100], cls_loss: 1.7251, transfer_loss: 0.1639, total_Loss: 1.7251, acc: 0.6486\n","lambda= 0.0 Epoch: [28/100], cls_loss: 1.6962, transfer_loss: 0.1652, total_Loss: 1.6962, acc: 0.6968\n","lambda= 0.0 Epoch: [29/100], cls_loss: 1.5569, transfer_loss: 0.1613, total_Loss: 1.5569, acc: 0.6707\n","lambda= 0.0 Epoch: [30/100], cls_loss: 1.4699, transfer_loss: 0.1623, total_Loss: 1.4699, acc: 0.6305\n","lambda= 0.0 Epoch: [31/100], cls_loss: 1.5070, transfer_loss: 0.1646, total_Loss: 1.5070, acc: 0.6827\n","lambda= 0.0 Epoch: [32/100], cls_loss: 1.5671, transfer_loss: 0.1632, total_Loss: 1.5671, acc: 0.6928\n","lambda= 0.0 Epoch: [33/100], cls_loss: 1.3948, transfer_loss: 0.1650, total_Loss: 1.3948, acc: 0.6827\n","lambda= 0.0 Epoch: [34/100], cls_loss: 1.3885, transfer_loss: 0.1646, total_Loss: 1.3885, acc: 0.6727\n","lambda= 0.0 Epoch: [35/100], cls_loss: 1.3876, transfer_loss: 0.1640, total_Loss: 1.3876, acc: 0.6988\n","lambda= 0.0 Epoch: [36/100], cls_loss: 1.2748, transfer_loss: 0.1633, total_Loss: 1.2748, acc: 0.6787\n","lambda= 0.0 Epoch: [37/100], cls_loss: 1.2498, transfer_loss: 0.1642, total_Loss: 1.2498, acc: 0.6867\n","lambda= 0.0 Epoch: [38/100], cls_loss: 1.2384, transfer_loss: 0.1632, total_Loss: 1.2384, acc: 0.7329\n","lambda= 0.0 Epoch: [39/100], cls_loss: 1.1930, transfer_loss: 0.1633, total_Loss: 1.1930, acc: 0.7229\n","lambda= 0.0 Epoch: [40/100], cls_loss: 1.1733, transfer_loss: 0.1647, total_Loss: 1.1733, acc: 0.7169\n","lambda= 0.0 Epoch: [41/100], cls_loss: 1.1675, transfer_loss: 0.1624, total_Loss: 1.1675, acc: 0.7149\n","lambda= 0.0 Epoch: [42/100], cls_loss: 1.1287, transfer_loss: 0.1620, total_Loss: 1.1287, acc: 0.7008\n","lambda= 0.0 Epoch: [43/100], cls_loss: 1.1586, transfer_loss: 0.1648, total_Loss: 1.1586, acc: 0.7189\n","lambda= 0.0 Epoch: [44/100], cls_loss: 1.0984, transfer_loss: 0.1652, total_Loss: 1.0984, acc: 0.7068\n","lambda= 0.0 Epoch: [45/100], cls_loss: 1.1022, transfer_loss: 0.1659, total_Loss: 1.1022, acc: 0.7309\n","lambda= 0.0 Epoch: [46/100], cls_loss: 1.0413, transfer_loss: 0.1638, total_Loss: 1.0413, acc: 0.7088\n","lambda= 0.0 Epoch: [47/100], cls_loss: 1.0570, transfer_loss: 0.1623, total_Loss: 1.0570, acc: 0.7349\n","lambda= 0.0 Epoch: [48/100], cls_loss: 1.0794, transfer_loss: 0.1635, total_Loss: 1.0794, acc: 0.7410\n","lambda= 0.0 Epoch: [49/100], cls_loss: 1.0757, transfer_loss: 0.1627, total_Loss: 1.0757, acc: 0.7490\n","lambda= 0.0 Epoch: [50/100], cls_loss: 1.0716, transfer_loss: 0.1632, total_Loss: 1.0716, acc: 0.7329\n","lambda= 0.0 Epoch: [51/100], cls_loss: 0.9970, transfer_loss: 0.1635, total_Loss: 0.9970, acc: 0.7289\n","lambda= 0.0 Epoch: [52/100], cls_loss: 1.0091, transfer_loss: 0.1619, total_Loss: 1.0091, acc: 0.7349\n","lambda= 0.0 Epoch: [53/100], cls_loss: 0.9928, transfer_loss: 0.1649, total_Loss: 0.9928, acc: 0.7309\n","lambda= 0.0 Epoch: [54/100], cls_loss: 1.0027, transfer_loss: 0.1637, total_Loss: 1.0027, acc: 0.7470\n","lambda= 0.0 Epoch: [55/100], cls_loss: 0.9633, transfer_loss: 0.1656, total_Loss: 0.9633, acc: 0.7470\n","lambda= 0.0 Epoch: [56/100], cls_loss: 0.9760, transfer_loss: 0.1639, total_Loss: 0.9760, acc: 0.7369\n","lambda= 0.0 Epoch: [57/100], cls_loss: 1.0083, transfer_loss: 0.1639, total_Loss: 1.0083, acc: 0.7470\n","lambda= 0.0 Epoch: [58/100], cls_loss: 1.0312, transfer_loss: 0.1646, total_Loss: 1.0312, acc: 0.7450\n","lambda= 0.0 Epoch: [59/100], cls_loss: 0.9863, transfer_loss: 0.1647, total_Loss: 0.9863, acc: 0.7530\n","lambda= 0.0 Epoch: [60/100], cls_loss: 0.8753, transfer_loss: 0.1641, total_Loss: 0.8753, acc: 0.7430\n","lambda= 0.0 Epoch: [61/100], cls_loss: 0.8945, transfer_loss: 0.1653, total_Loss: 0.8945, acc: 0.7490\n","lambda= 0.0 Epoch: [62/100], cls_loss: 0.9527, transfer_loss: 0.1626, total_Loss: 0.9527, acc: 0.7510\n","lambda= 0.0 Epoch: [63/100], cls_loss: 0.8528, transfer_loss: 0.1629, total_Loss: 0.8528, acc: 0.7309\n","lambda= 0.0 Epoch: [64/100], cls_loss: 0.9082, transfer_loss: 0.1642, total_Loss: 0.9082, acc: 0.7490\n","lambda= 0.0 Epoch: [65/100], cls_loss: 0.8759, transfer_loss: 0.1640, total_Loss: 0.8759, acc: 0.7691\n","lambda= 0.0 Epoch: [66/100], cls_loss: 0.8069, transfer_loss: 0.1651, total_Loss: 0.8069, acc: 0.7530\n","lambda= 0.0 Epoch: [67/100], cls_loss: 0.8667, transfer_loss: 0.1630, total_Loss: 0.8667, acc: 0.7530\n","lambda= 0.0 Epoch: [68/100], cls_loss: 0.9062, transfer_loss: 0.1639, total_Loss: 0.9062, acc: 0.7249\n","lambda= 0.0 Epoch: [69/100], cls_loss: 0.8936, transfer_loss: 0.1630, total_Loss: 0.8936, acc: 0.7349\n","lambda= 0.0 Epoch: [70/100], cls_loss: 0.7938, transfer_loss: 0.1644, total_Loss: 0.7938, acc: 0.7410\n","lambda= 0.0 Epoch: [71/100], cls_loss: 0.8191, transfer_loss: 0.1642, total_Loss: 0.8191, acc: 0.7530\n","lambda= 0.0 Epoch: [72/100], cls_loss: 0.7788, transfer_loss: 0.1633, total_Loss: 0.7788, acc: 0.7771\n","lambda= 0.0 Epoch: [73/100], cls_loss: 0.7714, transfer_loss: 0.1652, total_Loss: 0.7714, acc: 0.7771\n","lambda= 0.0 Epoch: [74/100], cls_loss: 0.8275, transfer_loss: 0.1639, total_Loss: 0.8275, acc: 0.7731\n","lambda= 0.0 Epoch: [75/100], cls_loss: 0.7621, transfer_loss: 0.1623, total_Loss: 0.7621, acc: 0.7530\n","lambda= 0.0 Epoch: [76/100], cls_loss: 0.8942, transfer_loss: 0.1643, total_Loss: 0.8942, acc: 0.7610\n","lambda= 0.0 Epoch: [77/100], cls_loss: 0.7382, transfer_loss: 0.1629, total_Loss: 0.7382, acc: 0.7510\n","lambda= 0.0 Epoch: [78/100], cls_loss: 0.8302, transfer_loss: 0.1638, total_Loss: 0.8302, acc: 0.7651\n","lambda= 0.0 Epoch: [79/100], cls_loss: 0.7840, transfer_loss: 0.1637, total_Loss: 0.7840, acc: 0.7490\n","lambda= 0.0 Epoch: [80/100], cls_loss: 0.7842, transfer_loss: 0.1629, total_Loss: 0.7842, acc: 0.7550\n","lambda= 0.0 Epoch: [81/100], cls_loss: 0.8179, transfer_loss: 0.1638, total_Loss: 0.8179, acc: 0.7550\n","lambda= 0.0 Epoch: [82/100], cls_loss: 0.8313, transfer_loss: 0.1651, total_Loss: 0.8313, acc: 0.7470\n","lambda= 0.0 Epoch: [83/100], cls_loss: 0.7525, transfer_loss: 0.1623, total_Loss: 0.7525, acc: 0.7510\n","lambda= 0.0 Epoch: [84/100], cls_loss: 0.7152, transfer_loss: 0.1642, total_Loss: 0.7152, acc: 0.7751\n","lambda= 0.0 Epoch: [85/100], cls_loss: 0.8057, transfer_loss: 0.1658, total_Loss: 0.8057, acc: 0.7651\n","lambda= 0.0 Epoch: [86/100], cls_loss: 0.6715, transfer_loss: 0.1644, total_Loss: 0.6715, acc: 0.7731\n","lambda= 0.0 Epoch: [87/100], cls_loss: 0.7292, transfer_loss: 0.1646, total_Loss: 0.7292, acc: 0.7791\n","lambda= 0.0 Epoch: [88/100], cls_loss: 0.7228, transfer_loss: 0.1647, total_Loss: 0.7228, acc: 0.7530\n","lambda= 0.0 Epoch: [89/100], cls_loss: 0.7399, transfer_loss: 0.1631, total_Loss: 0.7399, acc: 0.7671\n","lambda= 0.0 Epoch: [90/100], cls_loss: 0.6570, transfer_loss: 0.1632, total_Loss: 0.6570, acc: 0.7550\n","lambda= 0.0 Epoch: [91/100], cls_loss: 0.6569, transfer_loss: 0.1632, total_Loss: 0.6569, acc: 0.7691\n","lambda= 0.0 Epoch: [92/100], cls_loss: 0.6849, transfer_loss: 0.1639, total_Loss: 0.6849, acc: 0.7871\n","lambda= 0.0 Epoch: [93/100], cls_loss: 0.6260, transfer_loss: 0.1647, total_Loss: 0.6260, acc: 0.7851\n","lambda= 0.0 Epoch: [94/100], cls_loss: 0.6579, transfer_loss: 0.1633, total_Loss: 0.6579, acc: 0.7590\n","lambda= 0.0 Epoch: [95/100], cls_loss: 0.7366, transfer_loss: 0.1655, total_Loss: 0.7366, acc: 0.7610\n","lambda= 0.0 Epoch: [96/100], cls_loss: 0.5959, transfer_loss: 0.1646, total_Loss: 0.5959, acc: 0.7751\n","lambda= 0.0 Epoch: [97/100], cls_loss: 0.6948, transfer_loss: 0.1652, total_Loss: 0.6948, acc: 0.7771\n","lambda= 0.0 Epoch: [98/100], cls_loss: 0.7025, transfer_loss: 0.1640, total_Loss: 0.7025, acc: 0.7831\n","lambda= 0.0 Epoch: [99/100], cls_loss: 0.7384, transfer_loss: 0.1632, total_Loss: 0.7384, acc: 0.7851\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.0 Test accuracy: 0.78714859437751\n","lambda= 0.1 Epoch: [ 0/100], cls_loss: 3.4261, transfer_loss: 0.1639, total_Loss: 3.4425, acc: 0.0663\n","lambda= 0.1 Epoch: [ 1/100], cls_loss: 3.4021, transfer_loss: 0.1641, total_Loss: 3.4186, acc: 0.0703\n","lambda= 0.1 Epoch: [ 2/100], cls_loss: 3.3884, transfer_loss: 0.1635, total_Loss: 3.4048, acc: 0.0763\n","lambda= 0.1 Epoch: [ 3/100], cls_loss: 3.3409, transfer_loss: 0.1635, total_Loss: 3.3572, acc: 0.1426\n","lambda= 0.1 Epoch: [ 4/100], cls_loss: 3.3241, transfer_loss: 0.1626, total_Loss: 3.3404, acc: 0.2651\n","lambda= 0.1 Epoch: [ 5/100], cls_loss: 3.2965, transfer_loss: 0.1632, total_Loss: 3.3128, acc: 0.3052\n","lambda= 0.1 Epoch: [ 6/100], cls_loss: 3.2482, transfer_loss: 0.1611, total_Loss: 3.2643, acc: 0.3454\n","lambda= 0.1 Epoch: [ 7/100], cls_loss: 3.2047, transfer_loss: 0.1642, total_Loss: 3.2212, acc: 0.3273\n","lambda= 0.1 Epoch: [ 8/100], cls_loss: 3.1737, transfer_loss: 0.1636, total_Loss: 3.1900, acc: 0.3474\n","lambda= 0.1 Epoch: [ 9/100], cls_loss: 3.1340, transfer_loss: 0.1649, total_Loss: 3.1505, acc: 0.4257\n","lambda= 0.1 Epoch: [10/100], cls_loss: 3.0742, transfer_loss: 0.1632, total_Loss: 3.0906, acc: 0.5020\n","lambda= 0.1 Epoch: [11/100], cls_loss: 3.0356, transfer_loss: 0.1632, total_Loss: 3.0519, acc: 0.5301\n","lambda= 0.1 Epoch: [12/100], cls_loss: 3.0001, transfer_loss: 0.1635, total_Loss: 3.0165, acc: 0.5522\n","lambda= 0.1 Epoch: [13/100], cls_loss: 2.8954, transfer_loss: 0.1621, total_Loss: 2.9116, acc: 0.5542\n","lambda= 0.1 Epoch: [14/100], cls_loss: 2.8175, transfer_loss: 0.1635, total_Loss: 2.8338, acc: 0.5361\n","lambda= 0.1 Epoch: [15/100], cls_loss: 2.7440, transfer_loss: 0.1648, total_Loss: 2.7605, acc: 0.5602\n","lambda= 0.1 Epoch: [16/100], cls_loss: 2.7012, transfer_loss: 0.1625, total_Loss: 2.7174, acc: 0.5622\n","lambda= 0.1 Epoch: [17/100], cls_loss: 2.6289, transfer_loss: 0.1643, total_Loss: 2.6453, acc: 0.5683\n","lambda= 0.1 Epoch: [18/100], cls_loss: 2.4910, transfer_loss: 0.1630, total_Loss: 2.5073, acc: 0.6205\n","lambda= 0.1 Epoch: [19/100], cls_loss: 2.3938, transfer_loss: 0.1619, total_Loss: 2.4100, acc: 0.6004\n","lambda= 0.1 Epoch: [20/100], cls_loss: 2.2316, transfer_loss: 0.1632, total_Loss: 2.2479, acc: 0.5663\n","lambda= 0.1 Epoch: [21/100], cls_loss: 2.2394, transfer_loss: 0.1636, total_Loss: 2.2558, acc: 0.6185\n","lambda= 0.1 Epoch: [22/100], cls_loss: 2.0962, transfer_loss: 0.1641, total_Loss: 2.1126, acc: 0.5783\n","lambda= 0.1 Epoch: [23/100], cls_loss: 1.9795, transfer_loss: 0.1641, total_Loss: 1.9959, acc: 0.6205\n","lambda= 0.1 Epoch: [24/100], cls_loss: 1.9560, transfer_loss: 0.1618, total_Loss: 1.9721, acc: 0.6285\n","lambda= 0.1 Epoch: [25/100], cls_loss: 1.8729, transfer_loss: 0.1641, total_Loss: 1.8893, acc: 0.6948\n","lambda= 0.1 Epoch: [26/100], cls_loss: 1.8040, transfer_loss: 0.1637, total_Loss: 1.8204, acc: 0.6827\n","lambda= 0.1 Epoch: [27/100], cls_loss: 1.7051, transfer_loss: 0.1640, total_Loss: 1.7215, acc: 0.6767\n","lambda= 0.1 Epoch: [28/100], cls_loss: 1.6480, transfer_loss: 0.1639, total_Loss: 1.6644, acc: 0.6827\n","lambda= 0.1 Epoch: [29/100], cls_loss: 1.5851, transfer_loss: 0.1646, total_Loss: 1.6015, acc: 0.6968\n","lambda= 0.1 Epoch: [30/100], cls_loss: 1.5571, transfer_loss: 0.1639, total_Loss: 1.5735, acc: 0.7028\n","lambda= 0.1 Epoch: [31/100], cls_loss: 1.5801, transfer_loss: 0.1629, total_Loss: 1.5964, acc: 0.6988\n","lambda= 0.1 Epoch: [32/100], cls_loss: 1.4953, transfer_loss: 0.1652, total_Loss: 1.5118, acc: 0.7189\n","lambda= 0.1 Epoch: [33/100], cls_loss: 1.3761, transfer_loss: 0.1620, total_Loss: 1.3922, acc: 0.7129\n","lambda= 0.1 Epoch: [34/100], cls_loss: 1.4195, transfer_loss: 0.1641, total_Loss: 1.4359, acc: 0.7028\n","lambda= 0.1 Epoch: [35/100], cls_loss: 1.3593, transfer_loss: 0.1651, total_Loss: 1.3758, acc: 0.7269\n","lambda= 0.1 Epoch: [36/100], cls_loss: 1.2958, transfer_loss: 0.1619, total_Loss: 1.3120, acc: 0.7028\n","lambda= 0.1 Epoch: [37/100], cls_loss: 1.3472, transfer_loss: 0.1622, total_Loss: 1.3635, acc: 0.7349\n","lambda= 0.1 Epoch: [38/100], cls_loss: 1.1994, transfer_loss: 0.1643, total_Loss: 1.2158, acc: 0.7169\n","lambda= 0.1 Epoch: [39/100], cls_loss: 1.3072, transfer_loss: 0.1640, total_Loss: 1.3236, acc: 0.7169\n","lambda= 0.1 Epoch: [40/100], cls_loss: 1.1923, transfer_loss: 0.1652, total_Loss: 1.2088, acc: 0.7169\n","lambda= 0.1 Epoch: [41/100], cls_loss: 1.2176, transfer_loss: 0.1636, total_Loss: 1.2340, acc: 0.7169\n","lambda= 0.1 Epoch: [42/100], cls_loss: 1.1301, transfer_loss: 0.1631, total_Loss: 1.1464, acc: 0.7329\n","lambda= 0.1 Epoch: [43/100], cls_loss: 1.1500, transfer_loss: 0.1639, total_Loss: 1.1664, acc: 0.7570\n","lambda= 0.1 Epoch: [44/100], cls_loss: 1.0376, transfer_loss: 0.1621, total_Loss: 1.0538, acc: 0.7390\n","lambda= 0.1 Epoch: [45/100], cls_loss: 1.1778, transfer_loss: 0.1626, total_Loss: 1.1940, acc: 0.7329\n","lambda= 0.1 Epoch: [46/100], cls_loss: 1.0931, transfer_loss: 0.1618, total_Loss: 1.1093, acc: 0.7530\n","lambda= 0.1 Epoch: [47/100], cls_loss: 1.0223, transfer_loss: 0.1645, total_Loss: 1.0387, acc: 0.7490\n","lambda= 0.1 Epoch: [48/100], cls_loss: 0.9961, transfer_loss: 0.1621, total_Loss: 1.0123, acc: 0.7530\n","lambda= 0.1 Epoch: [49/100], cls_loss: 1.0715, transfer_loss: 0.1635, total_Loss: 1.0879, acc: 0.7329\n","lambda= 0.1 Epoch: [50/100], cls_loss: 1.0495, transfer_loss: 0.1625, total_Loss: 1.0658, acc: 0.7249\n","lambda= 0.1 Epoch: [51/100], cls_loss: 1.0291, transfer_loss: 0.1623, total_Loss: 1.0453, acc: 0.7430\n","lambda= 0.1 Epoch: [52/100], cls_loss: 1.0208, transfer_loss: 0.1625, total_Loss: 1.0371, acc: 0.7410\n","lambda= 0.1 Epoch: [53/100], cls_loss: 1.1242, transfer_loss: 0.1617, total_Loss: 1.1404, acc: 0.7510\n","lambda= 0.1 Epoch: [54/100], cls_loss: 0.9775, transfer_loss: 0.1633, total_Loss: 0.9939, acc: 0.7450\n","lambda= 0.1 Epoch: [55/100], cls_loss: 1.0133, transfer_loss: 0.1627, total_Loss: 1.0296, acc: 0.7309\n","lambda= 0.1 Epoch: [56/100], cls_loss: 0.9661, transfer_loss: 0.1637, total_Loss: 0.9825, acc: 0.7510\n","lambda= 0.1 Epoch: [57/100], cls_loss: 0.9460, transfer_loss: 0.1644, total_Loss: 0.9625, acc: 0.7490\n","lambda= 0.1 Epoch: [58/100], cls_loss: 0.7880, transfer_loss: 0.1637, total_Loss: 0.8044, acc: 0.7570\n","lambda= 0.1 Epoch: [59/100], cls_loss: 0.9276, transfer_loss: 0.1620, total_Loss: 0.9438, acc: 0.7510\n","lambda= 0.1 Epoch: [60/100], cls_loss: 0.8510, transfer_loss: 0.1621, total_Loss: 0.8672, acc: 0.7410\n","lambda= 0.1 Epoch: [61/100], cls_loss: 0.9073, transfer_loss: 0.1631, total_Loss: 0.9236, acc: 0.7530\n","lambda= 0.1 Epoch: [62/100], cls_loss: 0.9620, transfer_loss: 0.1637, total_Loss: 0.9784, acc: 0.7369\n","lambda= 0.1 Epoch: [63/100], cls_loss: 0.8923, transfer_loss: 0.1635, total_Loss: 0.9087, acc: 0.7530\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.1 Test accuracy: 0.7570281124497992\n","lambda= 0.2 Epoch: [ 0/100], cls_loss: 3.4340, transfer_loss: 0.1609, total_Loss: 3.4662, acc: 0.0683\n","lambda= 0.2 Epoch: [ 1/100], cls_loss: 3.4192, transfer_loss: 0.1650, total_Loss: 3.4522, acc: 0.1566\n","lambda= 0.2 Epoch: [ 2/100], cls_loss: 3.3901, transfer_loss: 0.1625, total_Loss: 3.4226, acc: 0.1245\n","lambda= 0.2 Epoch: [ 3/100], cls_loss: 3.3611, transfer_loss: 0.1630, total_Loss: 3.3937, acc: 0.1325\n","lambda= 0.2 Epoch: [ 4/100], cls_loss: 3.3295, transfer_loss: 0.1638, total_Loss: 3.3622, acc: 0.1747\n","lambda= 0.2 Epoch: [ 5/100], cls_loss: 3.2979, transfer_loss: 0.1636, total_Loss: 3.3306, acc: 0.1908\n","lambda= 0.2 Epoch: [ 6/100], cls_loss: 3.2769, transfer_loss: 0.1632, total_Loss: 3.3096, acc: 0.3092\n","lambda= 0.2 Epoch: [ 7/100], cls_loss: 3.2351, transfer_loss: 0.1658, total_Loss: 3.2683, acc: 0.3835\n","lambda= 0.2 Epoch: [ 8/100], cls_loss: 3.1866, transfer_loss: 0.1617, total_Loss: 3.2189, acc: 0.4337\n","lambda= 0.2 Epoch: [ 9/100], cls_loss: 3.1523, transfer_loss: 0.1638, total_Loss: 3.1851, acc: 0.4839\n","lambda= 0.2 Epoch: [10/100], cls_loss: 3.1345, transfer_loss: 0.1651, total_Loss: 3.1675, acc: 0.4900\n","lambda= 0.2 Epoch: [11/100], cls_loss: 3.0561, transfer_loss: 0.1609, total_Loss: 3.0883, acc: 0.4036\n","lambda= 0.2 Epoch: [12/100], cls_loss: 2.9747, transfer_loss: 0.1643, total_Loss: 3.0076, acc: 0.4438\n","lambda= 0.2 Epoch: [13/100], cls_loss: 2.9357, transfer_loss: 0.1629, total_Loss: 2.9682, acc: 0.4398\n","lambda= 0.2 Epoch: [14/100], cls_loss: 2.8699, transfer_loss: 0.1614, total_Loss: 2.9022, acc: 0.4498\n","lambda= 0.2 Epoch: [15/100], cls_loss: 2.7932, transfer_loss: 0.1620, total_Loss: 2.8256, acc: 0.4618\n","lambda= 0.2 Epoch: [16/100], cls_loss: 2.7343, transfer_loss: 0.1629, total_Loss: 2.7669, acc: 0.5040\n","lambda= 0.2 Epoch: [17/100], cls_loss: 2.6495, transfer_loss: 0.1630, total_Loss: 2.6821, acc: 0.5382\n","lambda= 0.2 Epoch: [18/100], cls_loss: 2.5307, transfer_loss: 0.1622, total_Loss: 2.5632, acc: 0.5361\n","lambda= 0.2 Epoch: [19/100], cls_loss: 2.4405, transfer_loss: 0.1661, total_Loss: 2.4737, acc: 0.5402\n","lambda= 0.2 Epoch: [20/100], cls_loss: 2.3774, transfer_loss: 0.1635, total_Loss: 2.4101, acc: 0.6124\n","lambda= 0.2 Epoch: [21/100], cls_loss: 2.2789, transfer_loss: 0.1658, total_Loss: 2.3121, acc: 0.6225\n","lambda= 0.2 Epoch: [22/100], cls_loss: 2.1250, transfer_loss: 0.1645, total_Loss: 2.1579, acc: 0.5663\n","lambda= 0.2 Epoch: [23/100], cls_loss: 2.1386, transfer_loss: 0.1634, total_Loss: 2.1713, acc: 0.6004\n","lambda= 0.2 Epoch: [24/100], cls_loss: 1.9681, transfer_loss: 0.1620, total_Loss: 2.0005, acc: 0.5944\n","lambda= 0.2 Epoch: [25/100], cls_loss: 1.9112, transfer_loss: 0.1652, total_Loss: 1.9443, acc: 0.6305\n","lambda= 0.2 Epoch: [26/100], cls_loss: 1.7539, transfer_loss: 0.1656, total_Loss: 1.7871, acc: 0.6767\n","lambda= 0.2 Epoch: [27/100], cls_loss: 1.8113, transfer_loss: 0.1628, total_Loss: 1.8439, acc: 0.6466\n","lambda= 0.2 Epoch: [28/100], cls_loss: 1.7106, transfer_loss: 0.1671, total_Loss: 1.7440, acc: 0.7008\n","lambda= 0.2 Epoch: [29/100], cls_loss: 1.6666, transfer_loss: 0.1623, total_Loss: 1.6990, acc: 0.6827\n","lambda= 0.2 Epoch: [30/100], cls_loss: 1.6028, transfer_loss: 0.1622, total_Loss: 1.6352, acc: 0.6888\n","lambda= 0.2 Epoch: [31/100], cls_loss: 1.5192, transfer_loss: 0.1622, total_Loss: 1.5516, acc: 0.7269\n","lambda= 0.2 Epoch: [32/100], cls_loss: 1.5357, transfer_loss: 0.1626, total_Loss: 1.5682, acc: 0.7269\n","lambda= 0.2 Epoch: [33/100], cls_loss: 1.4623, transfer_loss: 0.1626, total_Loss: 1.4948, acc: 0.7349\n","lambda= 0.2 Epoch: [34/100], cls_loss: 1.3971, transfer_loss: 0.1628, total_Loss: 1.4296, acc: 0.6888\n","lambda= 0.2 Epoch: [35/100], cls_loss: 1.2926, transfer_loss: 0.1642, total_Loss: 1.3255, acc: 0.6546\n","lambda= 0.2 Epoch: [36/100], cls_loss: 1.3438, transfer_loss: 0.1639, total_Loss: 1.3766, acc: 0.7209\n","lambda= 0.2 Epoch: [37/100], cls_loss: 1.3350, transfer_loss: 0.1645, total_Loss: 1.3679, acc: 0.7068\n","lambda= 0.2 Epoch: [38/100], cls_loss: 1.3174, transfer_loss: 0.1632, total_Loss: 1.3500, acc: 0.7229\n","lambda= 0.2 Epoch: [39/100], cls_loss: 1.2888, transfer_loss: 0.1653, total_Loss: 1.3219, acc: 0.7149\n","lambda= 0.2 Epoch: [40/100], cls_loss: 1.2013, transfer_loss: 0.1635, total_Loss: 1.2340, acc: 0.7390\n","lambda= 0.2 Epoch: [41/100], cls_loss: 1.1418, transfer_loss: 0.1631, total_Loss: 1.1744, acc: 0.7269\n","lambda= 0.2 Epoch: [42/100], cls_loss: 1.1648, transfer_loss: 0.1620, total_Loss: 1.1972, acc: 0.7329\n","lambda= 0.2 Epoch: [43/100], cls_loss: 1.1685, transfer_loss: 0.1613, total_Loss: 1.2008, acc: 0.7289\n","lambda= 0.2 Epoch: [44/100], cls_loss: 1.1301, transfer_loss: 0.1643, total_Loss: 1.1629, acc: 0.7329\n","lambda= 0.2 Epoch: [45/100], cls_loss: 1.0022, transfer_loss: 0.1630, total_Loss: 1.0348, acc: 0.7189\n","lambda= 0.2 Epoch: [46/100], cls_loss: 1.1883, transfer_loss: 0.1626, total_Loss: 1.2208, acc: 0.7430\n","lambda= 0.2 Epoch: [47/100], cls_loss: 1.0915, transfer_loss: 0.1616, total_Loss: 1.1238, acc: 0.7590\n","lambda= 0.2 Epoch: [48/100], cls_loss: 1.1049, transfer_loss: 0.1612, total_Loss: 1.1371, acc: 0.7490\n","lambda= 0.2 Epoch: [49/100], cls_loss: 0.9821, transfer_loss: 0.1628, total_Loss: 1.0146, acc: 0.7410\n","lambda= 0.2 Epoch: [50/100], cls_loss: 1.0310, transfer_loss: 0.1627, total_Loss: 1.0636, acc: 0.7289\n","lambda= 0.2 Epoch: [51/100], cls_loss: 1.0296, transfer_loss: 0.1622, total_Loss: 1.0620, acc: 0.7369\n","lambda= 0.2 Epoch: [52/100], cls_loss: 0.9655, transfer_loss: 0.1621, total_Loss: 0.9979, acc: 0.7550\n","lambda= 0.2 Epoch: [53/100], cls_loss: 1.0336, transfer_loss: 0.1622, total_Loss: 1.0660, acc: 0.7791\n","lambda= 0.2 Epoch: [54/100], cls_loss: 1.0158, transfer_loss: 0.1615, total_Loss: 1.0481, acc: 0.7410\n","lambda= 0.2 Epoch: [55/100], cls_loss: 0.9822, transfer_loss: 0.1609, total_Loss: 1.0144, acc: 0.7369\n","lambda= 0.2 Epoch: [56/100], cls_loss: 0.8762, transfer_loss: 0.1643, total_Loss: 0.9090, acc: 0.7369\n","lambda= 0.2 Epoch: [57/100], cls_loss: 0.9729, transfer_loss: 0.1617, total_Loss: 1.0053, acc: 0.7570\n","lambda= 0.2 Epoch: [58/100], cls_loss: 0.8254, transfer_loss: 0.1620, total_Loss: 0.8578, acc: 0.7631\n","lambda= 0.2 Epoch: [59/100], cls_loss: 0.9418, transfer_loss: 0.1608, total_Loss: 0.9739, acc: 0.7871\n","lambda= 0.2 Epoch: [60/100], cls_loss: 0.8482, transfer_loss: 0.1616, total_Loss: 0.8805, acc: 0.7871\n","lambda= 0.2 Epoch: [61/100], cls_loss: 0.8580, transfer_loss: 0.1618, total_Loss: 0.8903, acc: 0.7610\n","lambda= 0.2 Epoch: [62/100], cls_loss: 0.9239, transfer_loss: 0.1623, total_Loss: 0.9564, acc: 0.7450\n","lambda= 0.2 Epoch: [63/100], cls_loss: 0.8519, transfer_loss: 0.1646, total_Loss: 0.8848, acc: 0.7490\n","lambda= 0.2 Epoch: [64/100], cls_loss: 0.8152, transfer_loss: 0.1624, total_Loss: 0.8477, acc: 0.7470\n","lambda= 0.2 Epoch: [65/100], cls_loss: 0.8750, transfer_loss: 0.1639, total_Loss: 0.9078, acc: 0.7651\n","lambda= 0.2 Epoch: [66/100], cls_loss: 0.8476, transfer_loss: 0.1634, total_Loss: 0.8803, acc: 0.7831\n","lambda= 0.2 Epoch: [67/100], cls_loss: 0.8952, transfer_loss: 0.1628, total_Loss: 0.9277, acc: 0.7892\n","lambda= 0.2 Epoch: [68/100], cls_loss: 0.8464, transfer_loss: 0.1619, total_Loss: 0.8788, acc: 0.7791\n","lambda= 0.2 Epoch: [69/100], cls_loss: 0.8330, transfer_loss: 0.1628, total_Loss: 0.8656, acc: 0.7751\n","lambda= 0.2 Epoch: [70/100], cls_loss: 0.8416, transfer_loss: 0.1610, total_Loss: 0.8738, acc: 0.7771\n","lambda= 0.2 Epoch: [71/100], cls_loss: 0.7390, transfer_loss: 0.1625, total_Loss: 0.7715, acc: 0.7831\n","lambda= 0.2 Epoch: [72/100], cls_loss: 0.8678, transfer_loss: 0.1619, total_Loss: 0.9002, acc: 0.7791\n","lambda= 0.2 Epoch: [73/100], cls_loss: 0.7461, transfer_loss: 0.1627, total_Loss: 0.7787, acc: 0.7771\n","lambda= 0.2 Epoch: [74/100], cls_loss: 0.7291, transfer_loss: 0.1635, total_Loss: 0.7618, acc: 0.7811\n","lambda= 0.2 Epoch: [75/100], cls_loss: 0.7637, transfer_loss: 0.1609, total_Loss: 0.7959, acc: 0.7771\n","lambda= 0.2 Epoch: [76/100], cls_loss: 0.9349, transfer_loss: 0.1625, total_Loss: 0.9674, acc: 0.7791\n","lambda= 0.2 Epoch: [77/100], cls_loss: 0.8662, transfer_loss: 0.1615, total_Loss: 0.8985, acc: 0.7691\n","lambda= 0.2 Epoch: [78/100], cls_loss: 0.7179, transfer_loss: 0.1639, total_Loss: 0.7507, acc: 0.7711\n","lambda= 0.2 Epoch: [79/100], cls_loss: 0.7246, transfer_loss: 0.1599, total_Loss: 0.7566, acc: 0.7871\n","lambda= 0.2 Epoch: [80/100], cls_loss: 0.7941, transfer_loss: 0.1638, total_Loss: 0.8269, acc: 0.7972\n","lambda= 0.2 Epoch: [81/100], cls_loss: 0.7028, transfer_loss: 0.1641, total_Loss: 0.7356, acc: 0.7892\n","lambda= 0.2 Epoch: [82/100], cls_loss: 0.7444, transfer_loss: 0.1628, total_Loss: 0.7770, acc: 0.7711\n","lambda= 0.2 Epoch: [83/100], cls_loss: 0.8091, transfer_loss: 0.1621, total_Loss: 0.8415, acc: 0.7590\n","lambda= 0.2 Epoch: [84/100], cls_loss: 0.7781, transfer_loss: 0.1630, total_Loss: 0.8107, acc: 0.7651\n","lambda= 0.2 Epoch: [85/100], cls_loss: 0.7345, transfer_loss: 0.1623, total_Loss: 0.7670, acc: 0.7811\n","lambda= 0.2 Epoch: [86/100], cls_loss: 0.7484, transfer_loss: 0.1620, total_Loss: 0.7808, acc: 0.7530\n","lambda= 0.2 Epoch: [87/100], cls_loss: 0.7544, transfer_loss: 0.1605, total_Loss: 0.7865, acc: 0.7590\n","lambda= 0.2 Epoch: [88/100], cls_loss: 0.7205, transfer_loss: 0.1621, total_Loss: 0.7529, acc: 0.7651\n","lambda= 0.2 Epoch: [89/100], cls_loss: 0.7165, transfer_loss: 0.1625, total_Loss: 0.7490, acc: 0.7631\n","lambda= 0.2 Epoch: [90/100], cls_loss: 0.7932, transfer_loss: 0.1608, total_Loss: 0.8253, acc: 0.7590\n","lambda= 0.2 Epoch: [91/100], cls_loss: 0.7096, transfer_loss: 0.1634, total_Loss: 0.7423, acc: 0.7691\n","lambda= 0.2 Epoch: [92/100], cls_loss: 0.6578, transfer_loss: 0.1616, total_Loss: 0.6901, acc: 0.7871\n","lambda= 0.2 Epoch: [93/100], cls_loss: 0.6729, transfer_loss: 0.1607, total_Loss: 0.7050, acc: 0.7892\n","lambda= 0.2 Epoch: [94/100], cls_loss: 0.7073, transfer_loss: 0.1611, total_Loss: 0.7395, acc: 0.7671\n","lambda= 0.2 Epoch: [95/100], cls_loss: 0.7830, transfer_loss: 0.1614, total_Loss: 0.8153, acc: 0.7631\n","lambda= 0.2 Epoch: [96/100], cls_loss: 0.7057, transfer_loss: 0.1614, total_Loss: 0.7380, acc: 0.7791\n","lambda= 0.2 Epoch: [97/100], cls_loss: 0.6874, transfer_loss: 0.1597, total_Loss: 0.7194, acc: 0.7871\n","lambda= 0.2 Epoch: [98/100], cls_loss: 0.7278, transfer_loss: 0.1631, total_Loss: 0.7604, acc: 0.8012\n","lambda= 0.2 Epoch: [99/100], cls_loss: 0.6656, transfer_loss: 0.1606, total_Loss: 0.6977, acc: 0.7992\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.2 Test accuracy: 0.8012048192771084\n","lambda= 0.3 Epoch: [ 0/100], cls_loss: 3.4345, transfer_loss: 0.1652, total_Loss: 3.4841, acc: 0.0663\n","lambda= 0.3 Epoch: [ 1/100], cls_loss: 3.4032, transfer_loss: 0.1637, total_Loss: 3.4523, acc: 0.1185\n","lambda= 0.3 Epoch: [ 2/100], cls_loss: 3.3796, transfer_loss: 0.1631, total_Loss: 3.4285, acc: 0.1325\n","lambda= 0.3 Epoch: [ 3/100], cls_loss: 3.3553, transfer_loss: 0.1642, total_Loss: 3.4046, acc: 0.1606\n","lambda= 0.3 Epoch: [ 4/100], cls_loss: 3.3245, transfer_loss: 0.1629, total_Loss: 3.3734, acc: 0.1546\n","lambda= 0.3 Epoch: [ 5/100], cls_loss: 3.3013, transfer_loss: 0.1643, total_Loss: 3.3506, acc: 0.2390\n","lambda= 0.3 Epoch: [ 6/100], cls_loss: 3.2817, transfer_loss: 0.1624, total_Loss: 3.3304, acc: 0.3153\n","lambda= 0.3 Epoch: [ 7/100], cls_loss: 3.2380, transfer_loss: 0.1638, total_Loss: 3.2871, acc: 0.3414\n","lambda= 0.3 Epoch: [ 8/100], cls_loss: 3.1991, transfer_loss: 0.1633, total_Loss: 3.2481, acc: 0.3112\n","lambda= 0.3 Epoch: [ 9/100], cls_loss: 3.1752, transfer_loss: 0.1627, total_Loss: 3.2239, acc: 0.3052\n","lambda= 0.3 Epoch: [10/100], cls_loss: 3.0985, transfer_loss: 0.1616, total_Loss: 3.1470, acc: 0.2932\n","lambda= 0.3 Epoch: [11/100], cls_loss: 3.0681, transfer_loss: 0.1628, total_Loss: 3.1169, acc: 0.4116\n","lambda= 0.3 Epoch: [12/100], cls_loss: 2.9967, transfer_loss: 0.1628, total_Loss: 3.0456, acc: 0.4920\n","lambda= 0.3 Epoch: [13/100], cls_loss: 2.9691, transfer_loss: 0.1635, total_Loss: 3.0182, acc: 0.5301\n","lambda= 0.3 Epoch: [14/100], cls_loss: 2.9115, transfer_loss: 0.1622, total_Loss: 2.9602, acc: 0.5141\n","lambda= 0.3 Epoch: [15/100], cls_loss: 2.8257, transfer_loss: 0.1626, total_Loss: 2.8745, acc: 0.5602\n","lambda= 0.3 Epoch: [16/100], cls_loss: 2.7285, transfer_loss: 0.1646, total_Loss: 2.7779, acc: 0.6265\n","lambda= 0.3 Epoch: [17/100], cls_loss: 2.6294, transfer_loss: 0.1628, total_Loss: 2.6782, acc: 0.5944\n","lambda= 0.3 Epoch: [18/100], cls_loss: 2.5963, transfer_loss: 0.1613, total_Loss: 2.6447, acc: 0.5803\n","lambda= 0.3 Epoch: [19/100], cls_loss: 2.5117, transfer_loss: 0.1615, total_Loss: 2.5602, acc: 0.5703\n","lambda= 0.3 Epoch: [20/100], cls_loss: 2.3727, transfer_loss: 0.1635, total_Loss: 2.4217, acc: 0.5924\n","lambda= 0.3 Epoch: [21/100], cls_loss: 2.2944, transfer_loss: 0.1628, total_Loss: 2.3432, acc: 0.5924\n","lambda= 0.3 Epoch: [22/100], cls_loss: 2.2206, transfer_loss: 0.1622, total_Loss: 2.2692, acc: 0.5964\n","lambda= 0.3 Epoch: [23/100], cls_loss: 2.1102, transfer_loss: 0.1652, total_Loss: 2.1598, acc: 0.6245\n","lambda= 0.3 Epoch: [24/100], cls_loss: 2.0029, transfer_loss: 0.1632, total_Loss: 2.0519, acc: 0.6506\n","lambda= 0.3 Epoch: [25/100], cls_loss: 2.0537, transfer_loss: 0.1627, total_Loss: 2.1025, acc: 0.6647\n","lambda= 0.3 Epoch: [26/100], cls_loss: 1.9178, transfer_loss: 0.1602, total_Loss: 1.9659, acc: 0.6767\n","lambda= 0.3 Epoch: [27/100], cls_loss: 1.8085, transfer_loss: 0.1631, total_Loss: 1.8575, acc: 0.6908\n","lambda= 0.3 Epoch: [28/100], cls_loss: 1.7234, transfer_loss: 0.1622, total_Loss: 1.7720, acc: 0.6667\n","lambda= 0.3 Epoch: [29/100], cls_loss: 1.6434, transfer_loss: 0.1639, total_Loss: 1.6926, acc: 0.6627\n","lambda= 0.3 Epoch: [30/100], cls_loss: 1.5228, transfer_loss: 0.1650, total_Loss: 1.5723, acc: 0.6606\n","lambda= 0.3 Epoch: [31/100], cls_loss: 1.5696, transfer_loss: 0.1632, total_Loss: 1.6185, acc: 0.6606\n","lambda= 0.3 Epoch: [32/100], cls_loss: 1.4792, transfer_loss: 0.1640, total_Loss: 1.5284, acc: 0.6968\n","lambda= 0.3 Epoch: [33/100], cls_loss: 1.4573, transfer_loss: 0.1618, total_Loss: 1.5058, acc: 0.6948\n","lambda= 0.3 Epoch: [34/100], cls_loss: 1.4175, transfer_loss: 0.1653, total_Loss: 1.4671, acc: 0.7048\n","lambda= 0.3 Epoch: [35/100], cls_loss: 1.3054, transfer_loss: 0.1624, total_Loss: 1.3541, acc: 0.7149\n","lambda= 0.3 Epoch: [36/100], cls_loss: 1.3419, transfer_loss: 0.1644, total_Loss: 1.3912, acc: 0.7329\n","lambda= 0.3 Epoch: [37/100], cls_loss: 1.3204, transfer_loss: 0.1622, total_Loss: 1.3691, acc: 0.7189\n","lambda= 0.3 Epoch: [38/100], cls_loss: 1.2996, transfer_loss: 0.1635, total_Loss: 1.3487, acc: 0.7028\n","lambda= 0.3 Epoch: [39/100], cls_loss: 1.2919, transfer_loss: 0.1647, total_Loss: 1.3413, acc: 0.7189\n","lambda= 0.3 Epoch: [40/100], cls_loss: 1.1548, transfer_loss: 0.1656, total_Loss: 1.2044, acc: 0.7289\n","lambda= 0.3 Epoch: [41/100], cls_loss: 1.1850, transfer_loss: 0.1637, total_Loss: 1.2341, acc: 0.7369\n","lambda= 0.3 Epoch: [42/100], cls_loss: 1.1890, transfer_loss: 0.1611, total_Loss: 1.2373, acc: 0.7269\n","lambda= 0.3 Epoch: [43/100], cls_loss: 1.1601, transfer_loss: 0.1627, total_Loss: 1.2089, acc: 0.7088\n","lambda= 0.3 Epoch: [44/100], cls_loss: 1.2167, transfer_loss: 0.1621, total_Loss: 1.2654, acc: 0.7329\n","lambda= 0.3 Epoch: [45/100], cls_loss: 1.1270, transfer_loss: 0.1618, total_Loss: 1.1755, acc: 0.7108\n","lambda= 0.3 Epoch: [46/100], cls_loss: 1.0572, transfer_loss: 0.1639, total_Loss: 1.1063, acc: 0.7309\n","lambda= 0.3 Epoch: [47/100], cls_loss: 1.1178, transfer_loss: 0.1635, total_Loss: 1.1669, acc: 0.7149\n","lambda= 0.3 Epoch: [48/100], cls_loss: 1.0804, transfer_loss: 0.1635, total_Loss: 1.1295, acc: 0.7269\n","lambda= 0.3 Epoch: [49/100], cls_loss: 1.0849, transfer_loss: 0.1630, total_Loss: 1.1338, acc: 0.7631\n","lambda= 0.3 Epoch: [50/100], cls_loss: 1.1607, transfer_loss: 0.1639, total_Loss: 1.2098, acc: 0.7510\n","lambda= 0.3 Epoch: [51/100], cls_loss: 1.0003, transfer_loss: 0.1617, total_Loss: 1.0488, acc: 0.7410\n","lambda= 0.3 Epoch: [52/100], cls_loss: 0.9873, transfer_loss: 0.1627, total_Loss: 1.0362, acc: 0.7490\n","lambda= 0.3 Epoch: [53/100], cls_loss: 1.0322, transfer_loss: 0.1640, total_Loss: 1.0814, acc: 0.7450\n","lambda= 0.3 Epoch: [54/100], cls_loss: 0.9099, transfer_loss: 0.1634, total_Loss: 0.9589, acc: 0.7450\n","lambda= 0.3 Epoch: [55/100], cls_loss: 1.0419, transfer_loss: 0.1649, total_Loss: 1.0914, acc: 0.7450\n","lambda= 0.3 Epoch: [56/100], cls_loss: 0.8821, transfer_loss: 0.1619, total_Loss: 0.9307, acc: 0.7590\n","lambda= 0.3 Epoch: [57/100], cls_loss: 1.0346, transfer_loss: 0.1633, total_Loss: 1.0836, acc: 0.7470\n","lambda= 0.3 Epoch: [58/100], cls_loss: 0.8963, transfer_loss: 0.1629, total_Loss: 0.9451, acc: 0.7570\n","lambda= 0.3 Epoch: [59/100], cls_loss: 0.9417, transfer_loss: 0.1614, total_Loss: 0.9901, acc: 0.7791\n","lambda= 0.3 Epoch: [60/100], cls_loss: 0.9524, transfer_loss: 0.1636, total_Loss: 1.0015, acc: 0.7610\n","lambda= 0.3 Epoch: [61/100], cls_loss: 0.9202, transfer_loss: 0.1620, total_Loss: 0.9688, acc: 0.7490\n","lambda= 0.3 Epoch: [62/100], cls_loss: 0.9179, transfer_loss: 0.1655, total_Loss: 0.9676, acc: 0.7530\n","lambda= 0.3 Epoch: [63/100], cls_loss: 0.9549, transfer_loss: 0.1621, total_Loss: 1.0035, acc: 0.7390\n","lambda= 0.3 Epoch: [64/100], cls_loss: 0.9525, transfer_loss: 0.1639, total_Loss: 1.0016, acc: 0.7430\n","lambda= 0.3 Epoch: [65/100], cls_loss: 0.9164, transfer_loss: 0.1621, total_Loss: 0.9650, acc: 0.7309\n","lambda= 0.3 Epoch: [66/100], cls_loss: 0.8847, transfer_loss: 0.1629, total_Loss: 0.9336, acc: 0.7510\n","lambda= 0.3 Epoch: [67/100], cls_loss: 0.8185, transfer_loss: 0.1626, total_Loss: 0.8673, acc: 0.7550\n","lambda= 0.3 Epoch: [68/100], cls_loss: 0.8368, transfer_loss: 0.1630, total_Loss: 0.8857, acc: 0.7590\n","lambda= 0.3 Epoch: [69/100], cls_loss: 0.8357, transfer_loss: 0.1633, total_Loss: 0.8847, acc: 0.7811\n","lambda= 0.3 Epoch: [70/100], cls_loss: 0.8861, transfer_loss: 0.1619, total_Loss: 0.9346, acc: 0.7671\n","lambda= 0.3 Epoch: [71/100], cls_loss: 0.7875, transfer_loss: 0.1642, total_Loss: 0.8368, acc: 0.7671\n","lambda= 0.3 Epoch: [72/100], cls_loss: 0.7573, transfer_loss: 0.1625, total_Loss: 0.8061, acc: 0.7510\n","lambda= 0.3 Epoch: [73/100], cls_loss: 0.7558, transfer_loss: 0.1622, total_Loss: 0.8045, acc: 0.7711\n","lambda= 0.3 Epoch: [74/100], cls_loss: 0.8738, transfer_loss: 0.1606, total_Loss: 0.9220, acc: 0.7610\n","lambda= 0.3 Epoch: [75/100], cls_loss: 0.7384, transfer_loss: 0.1630, total_Loss: 0.7873, acc: 0.7651\n","lambda= 0.3 Epoch: [76/100], cls_loss: 0.7466, transfer_loss: 0.1626, total_Loss: 0.7954, acc: 0.7651\n","lambda= 0.3 Epoch: [77/100], cls_loss: 0.8254, transfer_loss: 0.1646, total_Loss: 0.8748, acc: 0.7851\n","lambda= 0.3 Epoch: [78/100], cls_loss: 0.7662, transfer_loss: 0.1607, total_Loss: 0.8144, acc: 0.7671\n","lambda= 0.3 Epoch: [79/100], cls_loss: 0.7112, transfer_loss: 0.1616, total_Loss: 0.7597, acc: 0.7590\n","lambda= 0.3 Epoch: [80/100], cls_loss: 0.6963, transfer_loss: 0.1607, total_Loss: 0.7445, acc: 0.7390\n","lambda= 0.3 Epoch: [81/100], cls_loss: 0.7350, transfer_loss: 0.1638, total_Loss: 0.7841, acc: 0.7751\n","lambda= 0.3 Epoch: [82/100], cls_loss: 0.7799, transfer_loss: 0.1613, total_Loss: 0.8283, acc: 0.7751\n","lambda= 0.3 Epoch: [83/100], cls_loss: 0.8215, transfer_loss: 0.1618, total_Loss: 0.8700, acc: 0.7831\n","lambda= 0.3 Epoch: [84/100], cls_loss: 0.7007, transfer_loss: 0.1628, total_Loss: 0.7496, acc: 0.7831\n","lambda= 0.3 Epoch: [85/100], cls_loss: 0.7515, transfer_loss: 0.1613, total_Loss: 0.7999, acc: 0.7871\n","lambda= 0.3 Epoch: [86/100], cls_loss: 0.7133, transfer_loss: 0.1618, total_Loss: 0.7619, acc: 0.7811\n","lambda= 0.3 Epoch: [87/100], cls_loss: 0.7858, transfer_loss: 0.1601, total_Loss: 0.8338, acc: 0.7912\n","lambda= 0.3 Epoch: [88/100], cls_loss: 0.7083, transfer_loss: 0.1598, total_Loss: 0.7563, acc: 0.7751\n","lambda= 0.3 Epoch: [89/100], cls_loss: 0.7811, transfer_loss: 0.1606, total_Loss: 0.8293, acc: 0.7811\n","lambda= 0.3 Epoch: [90/100], cls_loss: 0.7095, transfer_loss: 0.1602, total_Loss: 0.7575, acc: 0.7751\n","lambda= 0.3 Epoch: [91/100], cls_loss: 0.7957, transfer_loss: 0.1590, total_Loss: 0.8434, acc: 0.7751\n","lambda= 0.3 Epoch: [92/100], cls_loss: 0.6942, transfer_loss: 0.1604, total_Loss: 0.7424, acc: 0.7651\n","lambda= 0.3 Epoch: [93/100], cls_loss: 0.7184, transfer_loss: 0.1596, total_Loss: 0.7663, acc: 0.7631\n","lambda= 0.3 Epoch: [94/100], cls_loss: 0.6942, transfer_loss: 0.1603, total_Loss: 0.7423, acc: 0.7871\n","lambda= 0.3 Epoch: [95/100], cls_loss: 0.8133, transfer_loss: 0.1598, total_Loss: 0.8612, acc: 0.7811\n","lambda= 0.3 Epoch: [96/100], cls_loss: 0.6461, transfer_loss: 0.1599, total_Loss: 0.6941, acc: 0.7892\n","lambda= 0.3 Epoch: [97/100], cls_loss: 0.7032, transfer_loss: 0.1591, total_Loss: 0.7509, acc: 0.7631\n","lambda= 0.3 Epoch: [98/100], cls_loss: 0.6478, transfer_loss: 0.1624, total_Loss: 0.6965, acc: 0.7871\n","lambda= 0.3 Epoch: [99/100], cls_loss: 0.7144, transfer_loss: 0.1609, total_Loss: 0.7627, acc: 0.7831\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.30000000000000004 Test accuracy: 0.7911646586345381\n","lambda= 0.4 Epoch: [ 0/100], cls_loss: 3.4329, transfer_loss: 0.1656, total_Loss: 3.4991, acc: 0.0502\n","lambda= 0.4 Epoch: [ 1/100], cls_loss: 3.4111, transfer_loss: 0.1625, total_Loss: 3.4761, acc: 0.0823\n","lambda= 0.4 Epoch: [ 2/100], cls_loss: 3.3834, transfer_loss: 0.1656, total_Loss: 3.4496, acc: 0.1145\n","lambda= 0.4 Epoch: [ 3/100], cls_loss: 3.3514, transfer_loss: 0.1628, total_Loss: 3.4165, acc: 0.2088\n","lambda= 0.4 Epoch: [ 4/100], cls_loss: 3.3264, transfer_loss: 0.1634, total_Loss: 3.3918, acc: 0.2450\n","lambda= 0.4 Epoch: [ 5/100], cls_loss: 3.2908, transfer_loss: 0.1639, total_Loss: 3.3564, acc: 0.1827\n","lambda= 0.4 Epoch: [ 6/100], cls_loss: 3.2468, transfer_loss: 0.1622, total_Loss: 3.3116, acc: 0.2570\n","lambda= 0.4 Epoch: [ 7/100], cls_loss: 3.2257, transfer_loss: 0.1626, total_Loss: 3.2907, acc: 0.3133\n","lambda= 0.4 Epoch: [ 8/100], cls_loss: 3.1653, transfer_loss: 0.1639, total_Loss: 3.2309, acc: 0.3092\n","lambda= 0.4 Epoch: [ 9/100], cls_loss: 3.1446, transfer_loss: 0.1639, total_Loss: 3.2101, acc: 0.3916\n","lambda= 0.4 Epoch: [10/100], cls_loss: 3.0818, transfer_loss: 0.1638, total_Loss: 3.1473, acc: 0.4378\n","lambda= 0.4 Epoch: [11/100], cls_loss: 3.0074, transfer_loss: 0.1620, total_Loss: 3.0723, acc: 0.4398\n","lambda= 0.4 Epoch: [12/100], cls_loss: 2.9644, transfer_loss: 0.1628, total_Loss: 3.0295, acc: 0.4819\n","lambda= 0.4 Epoch: [13/100], cls_loss: 2.9097, transfer_loss: 0.1627, total_Loss: 2.9748, acc: 0.5080\n","lambda= 0.4 Epoch: [14/100], cls_loss: 2.8183, transfer_loss: 0.1643, total_Loss: 2.8840, acc: 0.4719\n","lambda= 0.4 Epoch: [15/100], cls_loss: 2.7463, transfer_loss: 0.1608, total_Loss: 2.8106, acc: 0.4699\n","lambda= 0.4 Epoch: [16/100], cls_loss: 2.6679, transfer_loss: 0.1628, total_Loss: 2.7330, acc: 0.5281\n","lambda= 0.4 Epoch: [17/100], cls_loss: 2.5908, transfer_loss: 0.1637, total_Loss: 2.6563, acc: 0.5843\n","lambda= 0.4 Epoch: [18/100], cls_loss: 2.4734, transfer_loss: 0.1619, total_Loss: 2.5382, acc: 0.5663\n","lambda= 0.4 Epoch: [19/100], cls_loss: 2.4149, transfer_loss: 0.1619, total_Loss: 2.4797, acc: 0.5582\n","lambda= 0.4 Epoch: [20/100], cls_loss: 2.3223, transfer_loss: 0.1623, total_Loss: 2.3872, acc: 0.5863\n","lambda= 0.4 Epoch: [21/100], cls_loss: 2.1946, transfer_loss: 0.1634, total_Loss: 2.2600, acc: 0.5884\n","lambda= 0.4 Epoch: [22/100], cls_loss: 2.2179, transfer_loss: 0.1636, total_Loss: 2.2833, acc: 0.5924\n","lambda= 0.4 Epoch: [23/100], cls_loss: 2.0661, transfer_loss: 0.1632, total_Loss: 2.1313, acc: 0.5863\n","lambda= 0.4 Epoch: [24/100], cls_loss: 1.9236, transfer_loss: 0.1625, total_Loss: 1.9886, acc: 0.6044\n","lambda= 0.4 Epoch: [25/100], cls_loss: 1.9292, transfer_loss: 0.1633, total_Loss: 1.9945, acc: 0.6145\n","lambda= 0.4 Epoch: [26/100], cls_loss: 1.8505, transfer_loss: 0.1629, total_Loss: 1.9156, acc: 0.6667\n","lambda= 0.4 Epoch: [27/100], cls_loss: 1.7866, transfer_loss: 0.1620, total_Loss: 1.8514, acc: 0.6647\n","lambda= 0.4 Epoch: [28/100], cls_loss: 1.6782, transfer_loss: 0.1627, total_Loss: 1.7432, acc: 0.6386\n","lambda= 0.4 Epoch: [29/100], cls_loss: 1.6334, transfer_loss: 0.1627, total_Loss: 1.6985, acc: 0.6386\n","lambda= 0.4 Epoch: [30/100], cls_loss: 1.5583, transfer_loss: 0.1617, total_Loss: 1.6230, acc: 0.6687\n","lambda= 0.4 Epoch: [31/100], cls_loss: 1.5035, transfer_loss: 0.1635, total_Loss: 1.5689, acc: 0.6707\n","lambda= 0.4 Epoch: [32/100], cls_loss: 1.4865, transfer_loss: 0.1645, total_Loss: 1.5523, acc: 0.6566\n","lambda= 0.4 Epoch: [33/100], cls_loss: 1.4007, transfer_loss: 0.1633, total_Loss: 1.4660, acc: 0.6546\n","lambda= 0.4 Epoch: [34/100], cls_loss: 1.4343, transfer_loss: 0.1616, total_Loss: 1.4989, acc: 0.7008\n","lambda= 0.4 Epoch: [35/100], cls_loss: 1.3348, transfer_loss: 0.1619, total_Loss: 1.3996, acc: 0.6968\n","lambda= 0.4 Epoch: [36/100], cls_loss: 1.3490, transfer_loss: 0.1628, total_Loss: 1.4141, acc: 0.7149\n","lambda= 0.4 Epoch: [37/100], cls_loss: 1.2200, transfer_loss: 0.1630, total_Loss: 1.2852, acc: 0.7169\n","lambda= 0.4 Epoch: [38/100], cls_loss: 1.2245, transfer_loss: 0.1625, total_Loss: 1.2895, acc: 0.6928\n","lambda= 0.4 Epoch: [39/100], cls_loss: 1.2352, transfer_loss: 0.1625, total_Loss: 1.3002, acc: 0.7129\n","lambda= 0.4 Epoch: [40/100], cls_loss: 1.1569, transfer_loss: 0.1616, total_Loss: 1.2216, acc: 0.6928\n","lambda= 0.4 Epoch: [41/100], cls_loss: 1.2260, transfer_loss: 0.1621, total_Loss: 1.2908, acc: 0.7088\n","lambda= 0.4 Epoch: [42/100], cls_loss: 1.1650, transfer_loss: 0.1633, total_Loss: 1.2304, acc: 0.7329\n","lambda= 0.4 Epoch: [43/100], cls_loss: 1.0914, transfer_loss: 0.1611, total_Loss: 1.1558, acc: 0.7249\n","lambda= 0.4 Epoch: [44/100], cls_loss: 1.1045, transfer_loss: 0.1609, total_Loss: 1.1688, acc: 0.7028\n","lambda= 0.4 Epoch: [45/100], cls_loss: 1.1367, transfer_loss: 0.1618, total_Loss: 1.2014, acc: 0.6908\n","lambda= 0.4 Epoch: [46/100], cls_loss: 1.0618, transfer_loss: 0.1641, total_Loss: 1.1274, acc: 0.6988\n","lambda= 0.4 Epoch: [47/100], cls_loss: 1.0922, transfer_loss: 0.1615, total_Loss: 1.1568, acc: 0.7309\n","lambda= 0.4 Epoch: [48/100], cls_loss: 1.0328, transfer_loss: 0.1623, total_Loss: 1.0977, acc: 0.7349\n","lambda= 0.4 Epoch: [49/100], cls_loss: 1.0172, transfer_loss: 0.1631, total_Loss: 1.0824, acc: 0.7309\n","lambda= 0.4 Epoch: [50/100], cls_loss: 0.9846, transfer_loss: 0.1622, total_Loss: 1.0495, acc: 0.7470\n","lambda= 0.4 Epoch: [51/100], cls_loss: 1.0104, transfer_loss: 0.1615, total_Loss: 1.0750, acc: 0.7490\n","lambda= 0.4 Epoch: [52/100], cls_loss: 1.0352, transfer_loss: 0.1602, total_Loss: 1.0993, acc: 0.7450\n","lambda= 0.4 Epoch: [53/100], cls_loss: 0.9587, transfer_loss: 0.1617, total_Loss: 1.0234, acc: 0.7390\n","lambda= 0.4 Epoch: [54/100], cls_loss: 1.0180, transfer_loss: 0.1611, total_Loss: 1.0824, acc: 0.7490\n","lambda= 0.4 Epoch: [55/100], cls_loss: 1.0563, transfer_loss: 0.1595, total_Loss: 1.1201, acc: 0.7550\n","lambda= 0.4 Epoch: [56/100], cls_loss: 1.0781, transfer_loss: 0.1603, total_Loss: 1.1422, acc: 0.7410\n","lambda= 0.4 Epoch: [57/100], cls_loss: 0.9435, transfer_loss: 0.1572, total_Loss: 1.0064, acc: 0.7390\n","lambda= 0.4 Epoch: [58/100], cls_loss: 0.9426, transfer_loss: 0.1585, total_Loss: 1.0060, acc: 0.7470\n","lambda= 0.4 Epoch: [59/100], cls_loss: 0.8641, transfer_loss: 0.1602, total_Loss: 0.9282, acc: 0.7450\n","lambda= 0.4 Epoch: [60/100], cls_loss: 0.9520, transfer_loss: 0.1604, total_Loss: 1.0161, acc: 0.7410\n","lambda= 0.4 Epoch: [61/100], cls_loss: 0.9836, transfer_loss: 0.1594, total_Loss: 1.0474, acc: 0.7510\n","lambda= 0.4 Epoch: [62/100], cls_loss: 0.9513, transfer_loss: 0.1587, total_Loss: 1.0147, acc: 0.7470\n","lambda= 0.4 Epoch: [63/100], cls_loss: 0.8593, transfer_loss: 0.1581, total_Loss: 0.9225, acc: 0.7369\n","lambda= 0.4 Epoch: [64/100], cls_loss: 0.8953, transfer_loss: 0.1586, total_Loss: 0.9587, acc: 0.7390\n","lambda= 0.4 Epoch: [65/100], cls_loss: 0.8401, transfer_loss: 0.1563, total_Loss: 0.9026, acc: 0.7590\n","lambda= 0.4 Epoch: [66/100], cls_loss: 0.8804, transfer_loss: 0.1578, total_Loss: 0.9435, acc: 0.7671\n","lambda= 0.4 Epoch: [67/100], cls_loss: 0.8727, transfer_loss: 0.1549, total_Loss: 0.9346, acc: 0.7530\n","lambda= 0.4 Epoch: [68/100], cls_loss: 0.8716, transfer_loss: 0.1536, total_Loss: 0.9330, acc: 0.7490\n","lambda= 0.4 Epoch: [69/100], cls_loss: 0.8660, transfer_loss: 0.1575, total_Loss: 0.9290, acc: 0.7691\n","lambda= 0.4 Epoch: [70/100], cls_loss: 0.7678, transfer_loss: 0.1535, total_Loss: 0.8292, acc: 0.7631\n","lambda= 0.4 Epoch: [71/100], cls_loss: 0.8757, transfer_loss: 0.1523, total_Loss: 0.9366, acc: 0.7851\n","lambda= 0.4 Epoch: [72/100], cls_loss: 0.7944, transfer_loss: 0.1541, total_Loss: 0.8561, acc: 0.7751\n","lambda= 0.4 Epoch: [73/100], cls_loss: 0.7750, transfer_loss: 0.1493, total_Loss: 0.8348, acc: 0.7851\n","lambda= 0.4 Epoch: [74/100], cls_loss: 0.8298, transfer_loss: 0.1515, total_Loss: 0.8904, acc: 0.7691\n","lambda= 0.4 Epoch: [75/100], cls_loss: 0.7527, transfer_loss: 0.1511, total_Loss: 0.8131, acc: 0.7490\n","lambda= 0.4 Epoch: [76/100], cls_loss: 0.7856, transfer_loss: 0.1499, total_Loss: 0.8456, acc: 0.7329\n","lambda= 0.4 Epoch: [77/100], cls_loss: 0.7859, transfer_loss: 0.1466, total_Loss: 0.8446, acc: 0.7691\n","lambda= 0.4 Epoch: [78/100], cls_loss: 0.7724, transfer_loss: 0.1481, total_Loss: 0.8316, acc: 0.7851\n","lambda= 0.4 Epoch: [79/100], cls_loss: 0.7645, transfer_loss: 0.1456, total_Loss: 0.8228, acc: 0.7771\n","lambda= 0.4 Epoch: [80/100], cls_loss: 0.8617, transfer_loss: 0.1428, total_Loss: 0.9188, acc: 0.7550\n","lambda= 0.4 Epoch: [81/100], cls_loss: 0.7111, transfer_loss: 0.1438, total_Loss: 0.7686, acc: 0.7771\n","lambda= 0.4 Epoch: [82/100], cls_loss: 0.7754, transfer_loss: 0.1416, total_Loss: 0.8320, acc: 0.7651\n","lambda= 0.4 Epoch: [83/100], cls_loss: 0.8153, transfer_loss: 0.1414, total_Loss: 0.8719, acc: 0.7550\n","lambda= 0.4 Epoch: [84/100], cls_loss: 0.7275, transfer_loss: 0.1379, total_Loss: 0.7826, acc: 0.7671\n","lambda= 0.4 Epoch: [85/100], cls_loss: 0.7490, transfer_loss: 0.1382, total_Loss: 0.8043, acc: 0.7610\n","lambda= 0.4 Epoch: [86/100], cls_loss: 0.8491, transfer_loss: 0.1368, total_Loss: 0.9038, acc: 0.7470\n","lambda= 0.4 Epoch: [87/100], cls_loss: 0.6797, transfer_loss: 0.1374, total_Loss: 0.7347, acc: 0.7871\n","lambda= 0.4 Epoch: [88/100], cls_loss: 0.7361, transfer_loss: 0.1358, total_Loss: 0.7904, acc: 0.7912\n","lambda= 0.4 Epoch: [89/100], cls_loss: 0.6054, transfer_loss: 0.1356, total_Loss: 0.6597, acc: 0.7851\n","lambda= 0.4 Epoch: [90/100], cls_loss: 0.7826, transfer_loss: 0.1363, total_Loss: 0.8371, acc: 0.7831\n","lambda= 0.4 Epoch: [91/100], cls_loss: 0.7405, transfer_loss: 0.1319, total_Loss: 0.7933, acc: 0.7871\n","lambda= 0.4 Epoch: [92/100], cls_loss: 0.6362, transfer_loss: 0.1326, total_Loss: 0.6892, acc: 0.7992\n","lambda= 0.4 Epoch: [93/100], cls_loss: 0.7224, transfer_loss: 0.1332, total_Loss: 0.7757, acc: 0.7731\n","lambda= 0.4 Epoch: [94/100], cls_loss: 0.7185, transfer_loss: 0.1313, total_Loss: 0.7710, acc: 0.7731\n","lambda= 0.4 Epoch: [95/100], cls_loss: 0.7144, transfer_loss: 0.1289, total_Loss: 0.7660, acc: 0.7952\n","lambda= 0.4 Epoch: [96/100], cls_loss: 0.6873, transfer_loss: 0.1322, total_Loss: 0.7401, acc: 0.7892\n","lambda= 0.4 Epoch: [97/100], cls_loss: 0.7774, transfer_loss: 0.1262, total_Loss: 0.8279, acc: 0.7731\n","lambda= 0.4 Epoch: [98/100], cls_loss: 0.7743, transfer_loss: 0.1259, total_Loss: 0.8247, acc: 0.7892\n","lambda= 0.4 Epoch: [99/100], cls_loss: 0.6840, transfer_loss: 0.1260, total_Loss: 0.7345, acc: 0.7831\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.4 Test accuracy: 0.7991967871485943\n","lambda= 0.5 Epoch: [ 0/100], cls_loss: 3.4359, transfer_loss: 0.1617, total_Loss: 3.5167, acc: 0.0763\n","lambda= 0.5 Epoch: [ 1/100], cls_loss: 3.4064, transfer_loss: 0.1629, total_Loss: 3.4879, acc: 0.1104\n","lambda= 0.5 Epoch: [ 2/100], cls_loss: 3.3737, transfer_loss: 0.1632, total_Loss: 3.4553, acc: 0.1265\n","lambda= 0.5 Epoch: [ 3/100], cls_loss: 3.3531, transfer_loss: 0.1644, total_Loss: 3.4353, acc: 0.2088\n","lambda= 0.5 Epoch: [ 4/100], cls_loss: 3.3277, transfer_loss: 0.1631, total_Loss: 3.4093, acc: 0.3213\n","lambda= 0.5 Epoch: [ 5/100], cls_loss: 3.2884, transfer_loss: 0.1626, total_Loss: 3.3697, acc: 0.3916\n","lambda= 0.5 Epoch: [ 6/100], cls_loss: 3.2607, transfer_loss: 0.1632, total_Loss: 3.3423, acc: 0.4217\n","lambda= 0.5 Epoch: [ 7/100], cls_loss: 3.2182, transfer_loss: 0.1642, total_Loss: 3.3003, acc: 0.4337\n","lambda= 0.5 Epoch: [ 8/100], cls_loss: 3.1796, transfer_loss: 0.1624, total_Loss: 3.2608, acc: 0.5020\n","lambda= 0.5 Epoch: [ 9/100], cls_loss: 3.1263, transfer_loss: 0.1649, total_Loss: 3.2087, acc: 0.5020\n","lambda= 0.5 Epoch: [10/100], cls_loss: 3.0764, transfer_loss: 0.1614, total_Loss: 3.1571, acc: 0.4157\n","lambda= 0.5 Epoch: [11/100], cls_loss: 3.0352, transfer_loss: 0.1648, total_Loss: 3.1176, acc: 0.4659\n","lambda= 0.5 Epoch: [12/100], cls_loss: 2.9829, transfer_loss: 0.1629, total_Loss: 3.0644, acc: 0.4297\n","lambda= 0.5 Epoch: [13/100], cls_loss: 2.8904, transfer_loss: 0.1630, total_Loss: 2.9719, acc: 0.5241\n","lambda= 0.5 Epoch: [14/100], cls_loss: 2.8283, transfer_loss: 0.1631, total_Loss: 2.9099, acc: 0.5301\n","lambda= 0.5 Epoch: [15/100], cls_loss: 2.7611, transfer_loss: 0.1640, total_Loss: 2.8431, acc: 0.5743\n","lambda= 0.5 Epoch: [16/100], cls_loss: 2.6760, transfer_loss: 0.1653, total_Loss: 2.7587, acc: 0.5141\n","lambda= 0.5 Epoch: [17/100], cls_loss: 2.5649, transfer_loss: 0.1618, total_Loss: 2.6458, acc: 0.5382\n","lambda= 0.5 Epoch: [18/100], cls_loss: 2.5278, transfer_loss: 0.1610, total_Loss: 2.6083, acc: 0.5622\n","lambda= 0.5 Epoch: [19/100], cls_loss: 2.4363, transfer_loss: 0.1627, total_Loss: 2.5177, acc: 0.5482\n","lambda= 0.5 Epoch: [20/100], cls_loss: 2.3068, transfer_loss: 0.1621, total_Loss: 2.3878, acc: 0.5562\n","lambda= 0.5 Epoch: [21/100], cls_loss: 2.2499, transfer_loss: 0.1631, total_Loss: 2.3315, acc: 0.5542\n","lambda= 0.5 Epoch: [22/100], cls_loss: 2.1623, transfer_loss: 0.1618, total_Loss: 2.2432, acc: 0.6024\n","lambda= 0.5 Epoch: [23/100], cls_loss: 2.1223, transfer_loss: 0.1619, total_Loss: 2.2032, acc: 0.6245\n","lambda= 0.5 Epoch: [24/100], cls_loss: 2.0219, transfer_loss: 0.1636, total_Loss: 2.1037, acc: 0.6386\n","lambda= 0.5 Epoch: [25/100], cls_loss: 1.9425, transfer_loss: 0.1630, total_Loss: 2.0240, acc: 0.6486\n","lambda= 0.5 Epoch: [26/100], cls_loss: 1.8432, transfer_loss: 0.1634, total_Loss: 1.9249, acc: 0.6526\n","lambda= 0.5 Epoch: [27/100], cls_loss: 1.7913, transfer_loss: 0.1625, total_Loss: 1.8726, acc: 0.6606\n","lambda= 0.5 Epoch: [28/100], cls_loss: 1.7075, transfer_loss: 0.1621, total_Loss: 1.7885, acc: 0.6968\n","lambda= 0.5 Epoch: [29/100], cls_loss: 1.6995, transfer_loss: 0.1615, total_Loss: 1.7803, acc: 0.7068\n","lambda= 0.5 Epoch: [30/100], cls_loss: 1.6452, transfer_loss: 0.1649, total_Loss: 1.7276, acc: 0.6968\n","lambda= 0.5 Epoch: [31/100], cls_loss: 1.5313, transfer_loss: 0.1628, total_Loss: 1.6127, acc: 0.6928\n","lambda= 0.5 Epoch: [32/100], cls_loss: 1.5688, transfer_loss: 0.1614, total_Loss: 1.6495, acc: 0.7289\n","lambda= 0.5 Epoch: [33/100], cls_loss: 1.5429, transfer_loss: 0.1641, total_Loss: 1.6249, acc: 0.6948\n","lambda= 0.5 Epoch: [34/100], cls_loss: 1.4069, transfer_loss: 0.1624, total_Loss: 1.4881, acc: 0.7149\n","lambda= 0.5 Epoch: [35/100], cls_loss: 1.4240, transfer_loss: 0.1627, total_Loss: 1.5053, acc: 0.7108\n","lambda= 0.5 Epoch: [36/100], cls_loss: 1.2205, transfer_loss: 0.1619, total_Loss: 1.3014, acc: 0.6968\n","lambda= 0.5 Epoch: [37/100], cls_loss: 1.3052, transfer_loss: 0.1633, total_Loss: 1.3868, acc: 0.7068\n","lambda= 0.5 Epoch: [38/100], cls_loss: 1.2430, transfer_loss: 0.1617, total_Loss: 1.3238, acc: 0.7530\n","lambda= 0.5 Epoch: [39/100], cls_loss: 1.2247, transfer_loss: 0.1635, total_Loss: 1.3065, acc: 0.7410\n","lambda= 0.5 Epoch: [40/100], cls_loss: 1.2657, transfer_loss: 0.1623, total_Loss: 1.3468, acc: 0.7129\n","lambda= 0.5 Epoch: [41/100], cls_loss: 1.2172, transfer_loss: 0.1611, total_Loss: 1.2978, acc: 0.6948\n","lambda= 0.5 Epoch: [42/100], cls_loss: 1.2425, transfer_loss: 0.1634, total_Loss: 1.3242, acc: 0.7390\n","lambda= 0.5 Epoch: [43/100], cls_loss: 1.1620, transfer_loss: 0.1611, total_Loss: 1.2426, acc: 0.7390\n","lambda= 0.5 Epoch: [44/100], cls_loss: 1.1513, transfer_loss: 0.1592, total_Loss: 1.2308, acc: 0.7450\n","lambda= 0.5 Epoch: [45/100], cls_loss: 1.1537, transfer_loss: 0.1624, total_Loss: 1.2350, acc: 0.7349\n","lambda= 0.5 Epoch: [46/100], cls_loss: 1.0563, transfer_loss: 0.1604, total_Loss: 1.1365, acc: 0.7289\n","lambda= 0.5 Epoch: [47/100], cls_loss: 1.0642, transfer_loss: 0.1591, total_Loss: 1.1437, acc: 0.7390\n","lambda= 0.5 Epoch: [48/100], cls_loss: 1.0737, transfer_loss: 0.1610, total_Loss: 1.1542, acc: 0.7530\n","lambda= 0.5 Epoch: [49/100], cls_loss: 1.1039, transfer_loss: 0.1594, total_Loss: 1.1836, acc: 0.7570\n","lambda= 0.5 Epoch: [50/100], cls_loss: 1.0007, transfer_loss: 0.1591, total_Loss: 1.0803, acc: 0.7530\n","lambda= 0.5 Epoch: [51/100], cls_loss: 1.0194, transfer_loss: 0.1573, total_Loss: 1.0981, acc: 0.7450\n","lambda= 0.5 Epoch: [52/100], cls_loss: 0.9751, transfer_loss: 0.1582, total_Loss: 1.0542, acc: 0.7309\n","lambda= 0.5 Epoch: [53/100], cls_loss: 0.9710, transfer_loss: 0.1591, total_Loss: 1.0505, acc: 0.7430\n","lambda= 0.5 Epoch: [54/100], cls_loss: 0.9661, transfer_loss: 0.1588, total_Loss: 1.0455, acc: 0.7470\n","lambda= 0.5 Epoch: [55/100], cls_loss: 0.9488, transfer_loss: 0.1583, total_Loss: 1.0280, acc: 0.7309\n","lambda= 0.5 Epoch: [56/100], cls_loss: 0.9228, transfer_loss: 0.1554, total_Loss: 1.0005, acc: 0.7149\n","lambda= 0.5 Epoch: [57/100], cls_loss: 0.9336, transfer_loss: 0.1544, total_Loss: 1.0108, acc: 0.7048\n","lambda= 0.5 Epoch: [58/100], cls_loss: 1.0082, transfer_loss: 0.1541, total_Loss: 1.0852, acc: 0.7450\n","lambda= 0.5 Epoch: [59/100], cls_loss: 0.9887, transfer_loss: 0.1544, total_Loss: 1.0659, acc: 0.7671\n","lambda= 0.5 Epoch: [60/100], cls_loss: 0.9257, transfer_loss: 0.1530, total_Loss: 1.0022, acc: 0.7791\n","lambda= 0.5 Epoch: [61/100], cls_loss: 0.8364, transfer_loss: 0.1495, total_Loss: 0.9111, acc: 0.7570\n","lambda= 0.5 Epoch: [62/100], cls_loss: 0.9126, transfer_loss: 0.1501, total_Loss: 0.9877, acc: 0.7711\n","lambda= 0.5 Epoch: [63/100], cls_loss: 0.8589, transfer_loss: 0.1458, total_Loss: 0.9318, acc: 0.7751\n","lambda= 0.5 Epoch: [64/100], cls_loss: 0.8499, transfer_loss: 0.1484, total_Loss: 0.9241, acc: 0.7631\n","lambda= 0.5 Epoch: [65/100], cls_loss: 0.8688, transfer_loss: 0.1456, total_Loss: 0.9416, acc: 0.7791\n","lambda= 0.5 Epoch: [66/100], cls_loss: 0.8862, transfer_loss: 0.1434, total_Loss: 0.9579, acc: 0.7751\n","lambda= 0.5 Epoch: [67/100], cls_loss: 0.8071, transfer_loss: 0.1453, total_Loss: 0.8797, acc: 0.7369\n","lambda= 0.5 Epoch: [68/100], cls_loss: 0.8091, transfer_loss: 0.1442, total_Loss: 0.8812, acc: 0.7610\n","lambda= 0.5 Epoch: [69/100], cls_loss: 0.8441, transfer_loss: 0.1424, total_Loss: 0.9153, acc: 0.7771\n","lambda= 0.5 Epoch: [70/100], cls_loss: 0.8407, transfer_loss: 0.1385, total_Loss: 0.9100, acc: 0.7651\n","lambda= 0.5 Epoch: [71/100], cls_loss: 0.8176, transfer_loss: 0.1361, total_Loss: 0.8857, acc: 0.7369\n","lambda= 0.5 Epoch: [72/100], cls_loss: 0.8023, transfer_loss: 0.1352, total_Loss: 0.8699, acc: 0.7450\n","lambda= 0.5 Epoch: [73/100], cls_loss: 0.8443, transfer_loss: 0.1350, total_Loss: 0.9118, acc: 0.7731\n","lambda= 0.5 Epoch: [74/100], cls_loss: 0.8296, transfer_loss: 0.1324, total_Loss: 0.8959, acc: 0.7711\n","lambda= 0.5 Epoch: [75/100], cls_loss: 0.7822, transfer_loss: 0.1272, total_Loss: 0.8458, acc: 0.7691\n","lambda= 0.5 Epoch: [76/100], cls_loss: 0.7909, transfer_loss: 0.1275, total_Loss: 0.8547, acc: 0.7450\n","lambda= 0.5 Epoch: [77/100], cls_loss: 0.7801, transfer_loss: 0.1256, total_Loss: 0.8429, acc: 0.7590\n","lambda= 0.5 Epoch: [78/100], cls_loss: 0.7675, transfer_loss: 0.1247, total_Loss: 0.8299, acc: 0.7831\n","lambda= 0.5 Epoch: [79/100], cls_loss: 0.7030, transfer_loss: 0.1249, total_Loss: 0.7654, acc: 0.7851\n","lambda= 0.5 Epoch: [80/100], cls_loss: 0.8392, transfer_loss: 0.1188, total_Loss: 0.8985, acc: 0.7831\n","lambda= 0.5 Epoch: [81/100], cls_loss: 0.7315, transfer_loss: 0.1201, total_Loss: 0.7916, acc: 0.7811\n","lambda= 0.5 Epoch: [82/100], cls_loss: 0.7274, transfer_loss: 0.1176, total_Loss: 0.7862, acc: 0.7811\n","lambda= 0.5 Epoch: [83/100], cls_loss: 0.7831, transfer_loss: 0.1164, total_Loss: 0.8413, acc: 0.7610\n","lambda= 0.5 Epoch: [84/100], cls_loss: 0.7384, transfer_loss: 0.1159, total_Loss: 0.7963, acc: 0.7711\n","lambda= 0.5 Epoch: [85/100], cls_loss: 0.7245, transfer_loss: 0.1129, total_Loss: 0.7810, acc: 0.7771\n","lambda= 0.5 Epoch: [86/100], cls_loss: 0.7523, transfer_loss: 0.1148, total_Loss: 0.8097, acc: 0.7751\n","lambda= 0.5 Epoch: [87/100], cls_loss: 0.6450, transfer_loss: 0.1089, total_Loss: 0.6995, acc: 0.7651\n","lambda= 0.5 Epoch: [88/100], cls_loss: 0.7020, transfer_loss: 0.1121, total_Loss: 0.7581, acc: 0.7610\n","lambda= 0.5 Epoch: [89/100], cls_loss: 0.7195, transfer_loss: 0.1074, total_Loss: 0.7732, acc: 0.7711\n","lambda= 0.5 Epoch: [90/100], cls_loss: 0.7367, transfer_loss: 0.1102, total_Loss: 0.7918, acc: 0.7871\n","lambda= 0.5 Epoch: [91/100], cls_loss: 0.6230, transfer_loss: 0.1072, total_Loss: 0.6766, acc: 0.7831\n","lambda= 0.5 Epoch: [92/100], cls_loss: 0.7232, transfer_loss: 0.1036, total_Loss: 0.7750, acc: 0.7751\n","lambda= 0.5 Epoch: [93/100], cls_loss: 0.6905, transfer_loss: 0.1067, total_Loss: 0.7439, acc: 0.7470\n","lambda= 0.5 Epoch: [94/100], cls_loss: 0.6173, transfer_loss: 0.1047, total_Loss: 0.6697, acc: 0.7610\n","lambda= 0.5 Epoch: [95/100], cls_loss: 0.8045, transfer_loss: 0.1040, total_Loss: 0.8565, acc: 0.7751\n","lambda= 0.5 Epoch: [96/100], cls_loss: 0.7232, transfer_loss: 0.1021, total_Loss: 0.7743, acc: 0.7731\n","lambda= 0.5 Epoch: [97/100], cls_loss: 0.6456, transfer_loss: 0.1003, total_Loss: 0.6958, acc: 0.7791\n","lambda= 0.5 Epoch: [98/100], cls_loss: 0.6763, transfer_loss: 0.0988, total_Loss: 0.7257, acc: 0.7791\n","lambda= 0.5 Epoch: [99/100], cls_loss: 0.7298, transfer_loss: 0.0980, total_Loss: 0.7788, acc: 0.7811\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.5 Test accuracy: 0.78714859437751\n","lambda= 0.6 Epoch: [ 0/100], cls_loss: 3.4374, transfer_loss: 0.1648, total_Loss: 3.5362, acc: 0.0562\n","lambda= 0.6 Epoch: [ 1/100], cls_loss: 3.4162, transfer_loss: 0.1623, total_Loss: 3.5136, acc: 0.1185\n","lambda= 0.6 Epoch: [ 2/100], cls_loss: 3.3922, transfer_loss: 0.1643, total_Loss: 3.4908, acc: 0.1606\n","lambda= 0.6 Epoch: [ 3/100], cls_loss: 3.3595, transfer_loss: 0.1632, total_Loss: 3.4574, acc: 0.1908\n","lambda= 0.6 Epoch: [ 4/100], cls_loss: 3.3370, transfer_loss: 0.1641, total_Loss: 3.4355, acc: 0.2269\n","lambda= 0.6 Epoch: [ 5/100], cls_loss: 3.2872, transfer_loss: 0.1642, total_Loss: 3.3857, acc: 0.1707\n","lambda= 0.6 Epoch: [ 6/100], cls_loss: 3.2521, transfer_loss: 0.1641, total_Loss: 3.3506, acc: 0.1145\n","lambda= 0.6 Epoch: [ 7/100], cls_loss: 3.2485, transfer_loss: 0.1644, total_Loss: 3.3472, acc: 0.2088\n","lambda= 0.6 Epoch: [ 8/100], cls_loss: 3.1995, transfer_loss: 0.1642, total_Loss: 3.2980, acc: 0.2309\n","lambda= 0.6 Epoch: [ 9/100], cls_loss: 3.1347, transfer_loss: 0.1633, total_Loss: 3.2326, acc: 0.3193\n","lambda= 0.6 Epoch: [10/100], cls_loss: 3.1103, transfer_loss: 0.1639, total_Loss: 3.2086, acc: 0.3655\n","lambda= 0.6 Epoch: [11/100], cls_loss: 3.0537, transfer_loss: 0.1631, total_Loss: 3.1515, acc: 0.3695\n","lambda= 0.6 Epoch: [12/100], cls_loss: 3.0006, transfer_loss: 0.1619, total_Loss: 3.0977, acc: 0.4438\n","lambda= 0.6 Epoch: [13/100], cls_loss: 2.9275, transfer_loss: 0.1622, total_Loss: 3.0248, acc: 0.4920\n","lambda= 0.6 Epoch: [14/100], cls_loss: 2.8833, transfer_loss: 0.1638, total_Loss: 2.9815, acc: 0.4438\n","lambda= 0.6 Epoch: [15/100], cls_loss: 2.8227, transfer_loss: 0.1623, total_Loss: 2.9200, acc: 0.4779\n","lambda= 0.6 Epoch: [16/100], cls_loss: 2.7103, transfer_loss: 0.1609, total_Loss: 2.8068, acc: 0.4739\n","lambda= 0.6 Epoch: [17/100], cls_loss: 2.6524, transfer_loss: 0.1646, total_Loss: 2.7511, acc: 0.5301\n","lambda= 0.6 Epoch: [18/100], cls_loss: 2.5382, transfer_loss: 0.1621, total_Loss: 2.6354, acc: 0.6064\n","lambda= 0.6 Epoch: [19/100], cls_loss: 2.4953, transfer_loss: 0.1616, total_Loss: 2.5923, acc: 0.6245\n","lambda= 0.6 Epoch: [20/100], cls_loss: 2.3290, transfer_loss: 0.1639, total_Loss: 2.4273, acc: 0.6145\n","lambda= 0.6 Epoch: [21/100], cls_loss: 2.2818, transfer_loss: 0.1629, total_Loss: 2.3795, acc: 0.6285\n","lambda= 0.6 Epoch: [22/100], cls_loss: 2.1514, transfer_loss: 0.1624, total_Loss: 2.2488, acc: 0.5964\n","lambda= 0.6 Epoch: [23/100], cls_loss: 2.1290, transfer_loss: 0.1635, total_Loss: 2.2271, acc: 0.6265\n","lambda= 0.6 Epoch: [24/100], cls_loss: 2.0543, transfer_loss: 0.1620, total_Loss: 2.1515, acc: 0.6908\n","lambda= 0.6 Epoch: [25/100], cls_loss: 1.9427, transfer_loss: 0.1616, total_Loss: 2.0396, acc: 0.6667\n","lambda= 0.6 Epoch: [26/100], cls_loss: 1.8583, transfer_loss: 0.1625, total_Loss: 1.9558, acc: 0.6506\n","lambda= 0.6 Epoch: [27/100], cls_loss: 1.7881, transfer_loss: 0.1621, total_Loss: 1.8854, acc: 0.6185\n","lambda= 0.6 Epoch: [28/100], cls_loss: 1.6196, transfer_loss: 0.1639, total_Loss: 1.7179, acc: 0.6446\n","lambda= 0.6 Epoch: [29/100], cls_loss: 1.6336, transfer_loss: 0.1625, total_Loss: 1.7311, acc: 0.6687\n","lambda= 0.6 Epoch: [30/100], cls_loss: 1.6020, transfer_loss: 0.1627, total_Loss: 1.6997, acc: 0.6767\n","lambda= 0.6 Epoch: [31/100], cls_loss: 1.5551, transfer_loss: 0.1624, total_Loss: 1.6525, acc: 0.6707\n","lambda= 0.6 Epoch: [32/100], cls_loss: 1.4952, transfer_loss: 0.1627, total_Loss: 1.5928, acc: 0.7048\n","lambda= 0.6 Epoch: [33/100], cls_loss: 1.4072, transfer_loss: 0.1617, total_Loss: 1.5043, acc: 0.6727\n","lambda= 0.6 Epoch: [34/100], cls_loss: 1.3725, transfer_loss: 0.1614, total_Loss: 1.4694, acc: 0.6948\n","lambda= 0.6 Epoch: [35/100], cls_loss: 1.4219, transfer_loss: 0.1620, total_Loss: 1.5191, acc: 0.6908\n","lambda= 0.6 Epoch: [36/100], cls_loss: 1.3656, transfer_loss: 0.1619, total_Loss: 1.4627, acc: 0.7068\n","lambda= 0.6 Epoch: [37/100], cls_loss: 1.3216, transfer_loss: 0.1642, total_Loss: 1.4201, acc: 0.7209\n","lambda= 0.6 Epoch: [38/100], cls_loss: 1.2647, transfer_loss: 0.1608, total_Loss: 1.3612, acc: 0.7189\n","lambda= 0.6 Epoch: [39/100], cls_loss: 1.2900, transfer_loss: 0.1609, total_Loss: 1.3865, acc: 0.7229\n","lambda= 0.6 Epoch: [40/100], cls_loss: 1.2428, transfer_loss: 0.1609, total_Loss: 1.3394, acc: 0.7349\n","lambda= 0.6 Epoch: [41/100], cls_loss: 1.2069, transfer_loss: 0.1614, total_Loss: 1.3037, acc: 0.7369\n","lambda= 0.6 Epoch: [42/100], cls_loss: 1.1496, transfer_loss: 0.1608, total_Loss: 1.2460, acc: 0.7309\n","lambda= 0.6 Epoch: [43/100], cls_loss: 1.0621, transfer_loss: 0.1606, total_Loss: 1.1584, acc: 0.7209\n","lambda= 0.6 Epoch: [44/100], cls_loss: 1.1721, transfer_loss: 0.1623, total_Loss: 1.2695, acc: 0.7189\n","lambda= 0.6 Epoch: [45/100], cls_loss: 1.1733, transfer_loss: 0.1602, total_Loss: 1.2694, acc: 0.7229\n","lambda= 0.6 Epoch: [46/100], cls_loss: 1.0532, transfer_loss: 0.1613, total_Loss: 1.1500, acc: 0.7008\n","lambda= 0.6 Epoch: [47/100], cls_loss: 1.0693, transfer_loss: 0.1586, total_Loss: 1.1644, acc: 0.7269\n","lambda= 0.6 Epoch: [48/100], cls_loss: 1.0041, transfer_loss: 0.1623, total_Loss: 1.1015, acc: 0.7450\n","lambda= 0.6 Epoch: [49/100], cls_loss: 1.0321, transfer_loss: 0.1587, total_Loss: 1.1272, acc: 0.7349\n","lambda= 0.6 Epoch: [50/100], cls_loss: 1.0413, transfer_loss: 0.1581, total_Loss: 1.1361, acc: 0.7169\n","lambda= 0.6 Epoch: [51/100], cls_loss: 1.0413, transfer_loss: 0.1584, total_Loss: 1.1364, acc: 0.7369\n","lambda= 0.6 Epoch: [52/100], cls_loss: 1.0393, transfer_loss: 0.1615, total_Loss: 1.1362, acc: 0.7530\n","lambda= 0.6 Epoch: [53/100], cls_loss: 0.9501, transfer_loss: 0.1576, total_Loss: 1.0447, acc: 0.7289\n","lambda= 0.6 Epoch: [54/100], cls_loss: 0.8419, transfer_loss: 0.1589, total_Loss: 0.9372, acc: 0.7249\n","lambda= 0.6 Epoch: [55/100], cls_loss: 0.9929, transfer_loss: 0.1575, total_Loss: 1.0874, acc: 0.7309\n","lambda= 0.6 Epoch: [56/100], cls_loss: 0.9263, transfer_loss: 0.1559, total_Loss: 1.0198, acc: 0.7289\n","lambda= 0.6 Epoch: [57/100], cls_loss: 0.9733, transfer_loss: 0.1552, total_Loss: 1.0665, acc: 0.7249\n","lambda= 0.6 Epoch: [58/100], cls_loss: 0.8926, transfer_loss: 0.1562, total_Loss: 0.9863, acc: 0.7189\n","lambda= 0.6 Epoch: [59/100], cls_loss: 0.9348, transfer_loss: 0.1557, total_Loss: 1.0282, acc: 0.7369\n","lambda= 0.6 Epoch: [60/100], cls_loss: 0.8961, transfer_loss: 0.1549, total_Loss: 0.9890, acc: 0.7189\n","lambda= 0.6 Epoch: [61/100], cls_loss: 0.8745, transfer_loss: 0.1545, total_Loss: 0.9672, acc: 0.7430\n","lambda= 0.6 Epoch: [62/100], cls_loss: 0.8483, transfer_loss: 0.1548, total_Loss: 0.9412, acc: 0.7490\n","lambda= 0.6 Epoch: [63/100], cls_loss: 0.9260, transfer_loss: 0.1542, total_Loss: 1.0185, acc: 0.7610\n","lambda= 0.6 Epoch: [64/100], cls_loss: 0.7947, transfer_loss: 0.1518, total_Loss: 0.8858, acc: 0.7691\n","lambda= 0.6 Epoch: [65/100], cls_loss: 0.9235, transfer_loss: 0.1508, total_Loss: 1.0140, acc: 0.7490\n","lambda= 0.6 Epoch: [66/100], cls_loss: 0.8976, transfer_loss: 0.1523, total_Loss: 0.9890, acc: 0.7369\n","lambda= 0.6 Epoch: [67/100], cls_loss: 0.8426, transfer_loss: 0.1493, total_Loss: 0.9322, acc: 0.7671\n","lambda= 0.6 Epoch: [68/100], cls_loss: 0.8496, transfer_loss: 0.1483, total_Loss: 0.9386, acc: 0.7590\n","lambda= 0.6 Epoch: [69/100], cls_loss: 0.9366, transfer_loss: 0.1482, total_Loss: 1.0256, acc: 0.7369\n","lambda= 0.6 Epoch: [70/100], cls_loss: 0.7637, transfer_loss: 0.1478, total_Loss: 0.8523, acc: 0.7289\n","lambda= 0.6 Epoch: [71/100], cls_loss: 0.8371, transfer_loss: 0.1452, total_Loss: 0.9242, acc: 0.7590\n","lambda= 0.6 Epoch: [72/100], cls_loss: 0.7662, transfer_loss: 0.1446, total_Loss: 0.8530, acc: 0.7610\n","lambda= 0.6 Epoch: [73/100], cls_loss: 0.8482, transfer_loss: 0.1471, total_Loss: 0.9364, acc: 0.7671\n","lambda= 0.6 Epoch: [74/100], cls_loss: 0.7628, transfer_loss: 0.1442, total_Loss: 0.8493, acc: 0.7590\n","lambda= 0.6 Epoch: [75/100], cls_loss: 0.8094, transfer_loss: 0.1434, total_Loss: 0.8955, acc: 0.7450\n","lambda= 0.6 Epoch: [76/100], cls_loss: 0.8208, transfer_loss: 0.1425, total_Loss: 0.9063, acc: 0.7450\n","lambda= 0.6 Epoch: [77/100], cls_loss: 0.7932, transfer_loss: 0.1407, total_Loss: 0.8776, acc: 0.7590\n","lambda= 0.6 Epoch: [78/100], cls_loss: 0.8669, transfer_loss: 0.1405, total_Loss: 0.9513, acc: 0.7731\n","lambda= 0.6 Epoch: [79/100], cls_loss: 0.7406, transfer_loss: 0.1377, total_Loss: 0.8232, acc: 0.7590\n","lambda= 0.6 Epoch: [80/100], cls_loss: 0.7938, transfer_loss: 0.1383, total_Loss: 0.8768, acc: 0.7831\n","lambda= 0.6 Epoch: [81/100], cls_loss: 0.8275, transfer_loss: 0.1341, total_Loss: 0.9080, acc: 0.7811\n","lambda= 0.6 Epoch: [82/100], cls_loss: 0.7200, transfer_loss: 0.1358, total_Loss: 0.8015, acc: 0.7691\n","lambda= 0.6 Epoch: [83/100], cls_loss: 0.7225, transfer_loss: 0.1343, total_Loss: 0.8031, acc: 0.7912\n","lambda= 0.6 Epoch: [84/100], cls_loss: 0.7610, transfer_loss: 0.1323, total_Loss: 0.8404, acc: 0.7992\n","lambda= 0.6 Epoch: [85/100], cls_loss: 0.7786, transfer_loss: 0.1325, total_Loss: 0.8581, acc: 0.7892\n","lambda= 0.6 Epoch: [86/100], cls_loss: 0.7720, transfer_loss: 0.1315, total_Loss: 0.8509, acc: 0.7751\n","lambda= 0.6 Epoch: [87/100], cls_loss: 0.7837, transfer_loss: 0.1304, total_Loss: 0.8619, acc: 0.7550\n","lambda= 0.6 Epoch: [88/100], cls_loss: 0.7004, transfer_loss: 0.1305, total_Loss: 0.7787, acc: 0.7811\n","lambda= 0.6 Epoch: [89/100], cls_loss: 0.7397, transfer_loss: 0.1296, total_Loss: 0.8175, acc: 0.7831\n","lambda= 0.6 Epoch: [90/100], cls_loss: 0.6667, transfer_loss: 0.1292, total_Loss: 0.7442, acc: 0.7711\n","lambda= 0.6 Epoch: [91/100], cls_loss: 0.6573, transfer_loss: 0.1235, total_Loss: 0.7314, acc: 0.7751\n","lambda= 0.6 Epoch: [92/100], cls_loss: 0.7524, transfer_loss: 0.1251, total_Loss: 0.8275, acc: 0.7871\n","lambda= 0.6 Epoch: [93/100], cls_loss: 0.8028, transfer_loss: 0.1228, total_Loss: 0.8765, acc: 0.7811\n","lambda= 0.6 Epoch: [94/100], cls_loss: 0.6984, transfer_loss: 0.1244, total_Loss: 0.7730, acc: 0.7932\n","lambda= 0.6 Epoch: [95/100], cls_loss: 0.6376, transfer_loss: 0.1211, total_Loss: 0.7103, acc: 0.7791\n","lambda= 0.6 Epoch: [96/100], cls_loss: 0.7059, transfer_loss: 0.1218, total_Loss: 0.7790, acc: 0.7711\n","lambda= 0.6 Epoch: [97/100], cls_loss: 0.7395, transfer_loss: 0.1187, total_Loss: 0.8108, acc: 0.7751\n","lambda= 0.6 Epoch: [98/100], cls_loss: 0.7358, transfer_loss: 0.1188, total_Loss: 0.8071, acc: 0.7671\n","lambda= 0.6 Epoch: [99/100], cls_loss: 0.7592, transfer_loss: 0.1210, total_Loss: 0.8318, acc: 0.7892\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.6000000000000001 Test accuracy: 0.7991967871485943\n","lambda= 0.7 Epoch: [ 0/100], cls_loss: 3.4279, transfer_loss: 0.1641, total_Loss: 3.5427, acc: 0.0462\n","lambda= 0.7 Epoch: [ 1/100], cls_loss: 3.4074, transfer_loss: 0.1640, total_Loss: 3.5222, acc: 0.0823\n","lambda= 0.7 Epoch: [ 2/100], cls_loss: 3.3803, transfer_loss: 0.1649, total_Loss: 3.4958, acc: 0.0542\n","lambda= 0.7 Epoch: [ 3/100], cls_loss: 3.3496, transfer_loss: 0.1635, total_Loss: 3.4641, acc: 0.0683\n","lambda= 0.7 Epoch: [ 4/100], cls_loss: 3.3243, transfer_loss: 0.1613, total_Loss: 3.4371, acc: 0.1827\n","lambda= 0.7 Epoch: [ 5/100], cls_loss: 3.2853, transfer_loss: 0.1624, total_Loss: 3.3990, acc: 0.2450\n","lambda= 0.7 Epoch: [ 6/100], cls_loss: 3.2493, transfer_loss: 0.1629, total_Loss: 3.3633, acc: 0.2329\n","lambda= 0.7 Epoch: [ 7/100], cls_loss: 3.2215, transfer_loss: 0.1647, total_Loss: 3.3368, acc: 0.2490\n","lambda= 0.7 Epoch: [ 8/100], cls_loss: 3.1895, transfer_loss: 0.1641, total_Loss: 3.3044, acc: 0.3896\n","lambda= 0.7 Epoch: [ 9/100], cls_loss: 3.1380, transfer_loss: 0.1642, total_Loss: 3.2530, acc: 0.4618\n","lambda= 0.7 Epoch: [10/100], cls_loss: 3.0827, transfer_loss: 0.1629, total_Loss: 3.1967, acc: 0.4699\n","lambda= 0.7 Epoch: [11/100], cls_loss: 3.0500, transfer_loss: 0.1629, total_Loss: 3.1640, acc: 0.5141\n","lambda= 0.7 Epoch: [12/100], cls_loss: 3.0025, transfer_loss: 0.1639, total_Loss: 3.1172, acc: 0.5301\n","lambda= 0.7 Epoch: [13/100], cls_loss: 2.9391, transfer_loss: 0.1634, total_Loss: 3.0535, acc: 0.5281\n","lambda= 0.7 Epoch: [14/100], cls_loss: 2.8564, transfer_loss: 0.1623, total_Loss: 2.9700, acc: 0.4859\n","lambda= 0.7 Epoch: [15/100], cls_loss: 2.7820, transfer_loss: 0.1621, total_Loss: 2.8955, acc: 0.5040\n","lambda= 0.7 Epoch: [16/100], cls_loss: 2.6599, transfer_loss: 0.1623, total_Loss: 2.7735, acc: 0.6124\n","lambda= 0.7 Epoch: [17/100], cls_loss: 2.6074, transfer_loss: 0.1632, total_Loss: 2.7217, acc: 0.5823\n","lambda= 0.7 Epoch: [18/100], cls_loss: 2.5633, transfer_loss: 0.1627, total_Loss: 2.6772, acc: 0.5683\n","lambda= 0.7 Epoch: [19/100], cls_loss: 2.4792, transfer_loss: 0.1617, total_Loss: 2.5923, acc: 0.5502\n","lambda= 0.7 Epoch: [20/100], cls_loss: 2.3622, transfer_loss: 0.1633, total_Loss: 2.4765, acc: 0.5843\n","lambda= 0.7 Epoch: [21/100], cls_loss: 2.1987, transfer_loss: 0.1627, total_Loss: 2.3126, acc: 0.5884\n","lambda= 0.7 Epoch: [22/100], cls_loss: 2.1645, transfer_loss: 0.1617, total_Loss: 2.2777, acc: 0.6205\n","lambda= 0.7 Epoch: [23/100], cls_loss: 2.1100, transfer_loss: 0.1628, total_Loss: 2.2240, acc: 0.6024\n","lambda= 0.7 Epoch: [24/100], cls_loss: 2.0295, transfer_loss: 0.1629, total_Loss: 2.1435, acc: 0.6506\n","lambda= 0.7 Epoch: [25/100], cls_loss: 1.9369, transfer_loss: 0.1612, total_Loss: 2.0497, acc: 0.6586\n","lambda= 0.7 Epoch: [26/100], cls_loss: 1.8393, transfer_loss: 0.1632, total_Loss: 1.9535, acc: 0.5863\n","lambda= 0.7 Epoch: [27/100], cls_loss: 1.7535, transfer_loss: 0.1620, total_Loss: 1.8669, acc: 0.6345\n","lambda= 0.7 Epoch: [28/100], cls_loss: 1.7354, transfer_loss: 0.1619, total_Loss: 1.8487, acc: 0.6727\n","lambda= 0.7 Epoch: [29/100], cls_loss: 1.6795, transfer_loss: 0.1606, total_Loss: 1.7919, acc: 0.6325\n","lambda= 0.7 Epoch: [30/100], cls_loss: 1.6113, transfer_loss: 0.1615, total_Loss: 1.7243, acc: 0.6928\n","lambda= 0.7 Epoch: [31/100], cls_loss: 1.5142, transfer_loss: 0.1600, total_Loss: 1.6262, acc: 0.6707\n","lambda= 0.7 Epoch: [32/100], cls_loss: 1.4589, transfer_loss: 0.1613, total_Loss: 1.5718, acc: 0.6687\n","lambda= 0.7 Epoch: [33/100], cls_loss: 1.4932, transfer_loss: 0.1593, total_Loss: 1.6047, acc: 0.6807\n","lambda= 0.7 Epoch: [34/100], cls_loss: 1.4303, transfer_loss: 0.1589, total_Loss: 1.5415, acc: 0.6948\n","lambda= 0.7 Epoch: [35/100], cls_loss: 1.4234, transfer_loss: 0.1575, total_Loss: 1.5337, acc: 0.7028\n","lambda= 0.7 Epoch: [36/100], cls_loss: 1.3250, transfer_loss: 0.1585, total_Loss: 1.4359, acc: 0.7249\n","lambda= 0.7 Epoch: [37/100], cls_loss: 1.2956, transfer_loss: 0.1575, total_Loss: 1.4059, acc: 0.7169\n","lambda= 0.7 Epoch: [38/100], cls_loss: 1.2670, transfer_loss: 0.1587, total_Loss: 1.3781, acc: 0.7129\n","lambda= 0.7 Epoch: [39/100], cls_loss: 1.3096, transfer_loss: 0.1584, total_Loss: 1.4205, acc: 0.7008\n","lambda= 0.7 Epoch: [40/100], cls_loss: 1.2728, transfer_loss: 0.1550, total_Loss: 1.3813, acc: 0.6908\n","lambda= 0.7 Epoch: [41/100], cls_loss: 1.2162, transfer_loss: 0.1547, total_Loss: 1.3245, acc: 0.7008\n","lambda= 0.7 Epoch: [42/100], cls_loss: 1.1818, transfer_loss: 0.1548, total_Loss: 1.2901, acc: 0.6968\n","lambda= 0.7 Epoch: [43/100], cls_loss: 1.1598, transfer_loss: 0.1537, total_Loss: 1.2675, acc: 0.7149\n","lambda= 0.7 Epoch: [44/100], cls_loss: 1.1829, transfer_loss: 0.1539, total_Loss: 1.2906, acc: 0.7149\n","lambda= 0.7 Epoch: [45/100], cls_loss: 1.0614, transfer_loss: 0.1515, total_Loss: 1.1674, acc: 0.7410\n","lambda= 0.7 Epoch: [46/100], cls_loss: 1.1919, transfer_loss: 0.1492, total_Loss: 1.2963, acc: 0.7169\n","lambda= 0.7 Epoch: [47/100], cls_loss: 1.0785, transfer_loss: 0.1489, total_Loss: 1.1828, acc: 0.7048\n","lambda= 0.7 Epoch: [48/100], cls_loss: 1.1023, transfer_loss: 0.1453, total_Loss: 1.2040, acc: 0.7169\n","lambda= 0.7 Epoch: [49/100], cls_loss: 1.0437, transfer_loss: 0.1452, total_Loss: 1.1453, acc: 0.7189\n","lambda= 0.7 Epoch: [50/100], cls_loss: 0.9712, transfer_loss: 0.1429, total_Loss: 1.0713, acc: 0.7229\n","lambda= 0.7 Epoch: [51/100], cls_loss: 0.9550, transfer_loss: 0.1432, total_Loss: 1.0552, acc: 0.7490\n","lambda= 0.7 Epoch: [52/100], cls_loss: 1.0972, transfer_loss: 0.1411, total_Loss: 1.1960, acc: 0.7349\n","lambda= 0.7 Epoch: [53/100], cls_loss: 1.0024, transfer_loss: 0.1400, total_Loss: 1.1004, acc: 0.7349\n","lambda= 0.7 Epoch: [54/100], cls_loss: 1.1010, transfer_loss: 0.1375, total_Loss: 1.1973, acc: 0.7430\n","lambda= 0.7 Epoch: [55/100], cls_loss: 1.0322, transfer_loss: 0.1346, total_Loss: 1.1263, acc: 0.7430\n","lambda= 0.7 Epoch: [56/100], cls_loss: 0.9367, transfer_loss: 0.1351, total_Loss: 1.0313, acc: 0.7390\n","lambda= 0.7 Epoch: [57/100], cls_loss: 0.9500, transfer_loss: 0.1311, total_Loss: 1.0417, acc: 0.7430\n","lambda= 0.7 Epoch: [58/100], cls_loss: 0.9197, transfer_loss: 0.1301, total_Loss: 1.0107, acc: 0.7470\n","lambda= 0.7 Epoch: [59/100], cls_loss: 0.9041, transfer_loss: 0.1289, total_Loss: 0.9944, acc: 0.7550\n","lambda= 0.7 Epoch: [60/100], cls_loss: 0.9848, transfer_loss: 0.1282, total_Loss: 1.0746, acc: 0.7530\n","lambda= 0.7 Epoch: [61/100], cls_loss: 0.9207, transfer_loss: 0.1242, total_Loss: 1.0076, acc: 0.7631\n","lambda= 0.7 Epoch: [62/100], cls_loss: 0.9352, transfer_loss: 0.1249, total_Loss: 1.0226, acc: 0.7570\n","lambda= 0.7 Epoch: [63/100], cls_loss: 0.9601, transfer_loss: 0.1214, total_Loss: 1.0450, acc: 0.7289\n","lambda= 0.7 Epoch: [64/100], cls_loss: 0.8566, transfer_loss: 0.1198, total_Loss: 0.9405, acc: 0.7610\n","lambda= 0.7 Epoch: [65/100], cls_loss: 0.9003, transfer_loss: 0.1189, total_Loss: 0.9835, acc: 0.7410\n","lambda= 0.7 Epoch: [66/100], cls_loss: 0.9349, transfer_loss: 0.1183, total_Loss: 1.0177, acc: 0.7550\n","lambda= 0.7 Epoch: [67/100], cls_loss: 0.9459, transfer_loss: 0.1182, total_Loss: 1.0286, acc: 0.7570\n","lambda= 0.7 Epoch: [68/100], cls_loss: 0.8696, transfer_loss: 0.1168, total_Loss: 0.9514, acc: 0.7510\n","lambda= 0.7 Epoch: [69/100], cls_loss: 0.8601, transfer_loss: 0.1164, total_Loss: 0.9416, acc: 0.7771\n","lambda= 0.7 Epoch: [70/100], cls_loss: 0.8441, transfer_loss: 0.1146, total_Loss: 0.9244, acc: 0.7671\n","lambda= 0.7 Epoch: [71/100], cls_loss: 0.7804, transfer_loss: 0.1125, total_Loss: 0.8592, acc: 0.7570\n","lambda= 0.7 Epoch: [72/100], cls_loss: 0.8148, transfer_loss: 0.1114, total_Loss: 0.8927, acc: 0.7651\n","lambda= 0.7 Epoch: [73/100], cls_loss: 0.8583, transfer_loss: 0.1101, total_Loss: 0.9354, acc: 0.7450\n","lambda= 0.7 Epoch: [74/100], cls_loss: 0.7476, transfer_loss: 0.1109, total_Loss: 0.8252, acc: 0.7490\n","lambda= 0.7 Epoch: [75/100], cls_loss: 0.8608, transfer_loss: 0.1055, total_Loss: 0.9346, acc: 0.7751\n","lambda= 0.7 Epoch: [76/100], cls_loss: 0.7489, transfer_loss: 0.1105, total_Loss: 0.8262, acc: 0.7751\n","lambda= 0.7 Epoch: [77/100], cls_loss: 0.8467, transfer_loss: 0.1026, total_Loss: 0.9185, acc: 0.7871\n","lambda= 0.7 Epoch: [78/100], cls_loss: 0.7217, transfer_loss: 0.1043, total_Loss: 0.7947, acc: 0.7811\n","lambda= 0.7 Epoch: [79/100], cls_loss: 0.7560, transfer_loss: 0.1040, total_Loss: 0.8288, acc: 0.7851\n","lambda= 0.7 Epoch: [80/100], cls_loss: 0.7535, transfer_loss: 0.1031, total_Loss: 0.8257, acc: 0.7972\n","lambda= 0.7 Epoch: [81/100], cls_loss: 0.8186, transfer_loss: 0.0991, total_Loss: 0.8880, acc: 0.7651\n","lambda= 0.7 Epoch: [82/100], cls_loss: 0.7614, transfer_loss: 0.0994, total_Loss: 0.8310, acc: 0.7791\n","lambda= 0.7 Epoch: [83/100], cls_loss: 0.7681, transfer_loss: 0.1005, total_Loss: 0.8384, acc: 0.7892\n","lambda= 0.7 Epoch: [84/100], cls_loss: 0.7569, transfer_loss: 0.1002, total_Loss: 0.8270, acc: 0.7751\n","lambda= 0.7 Epoch: [85/100], cls_loss: 0.6986, transfer_loss: 0.0972, total_Loss: 0.7667, acc: 0.7711\n","lambda= 0.7 Epoch: [86/100], cls_loss: 0.7663, transfer_loss: 0.0958, total_Loss: 0.8334, acc: 0.7811\n","lambda= 0.7 Epoch: [87/100], cls_loss: 0.7185, transfer_loss: 0.0977, total_Loss: 0.7869, acc: 0.7892\n","lambda= 0.7 Epoch: [88/100], cls_loss: 0.7835, transfer_loss: 0.0991, total_Loss: 0.8529, acc: 0.7811\n","lambda= 0.7 Epoch: [89/100], cls_loss: 0.7175, transfer_loss: 0.0955, total_Loss: 0.7844, acc: 0.8012\n","lambda= 0.7 Epoch: [90/100], cls_loss: 0.6324, transfer_loss: 0.0945, total_Loss: 0.6985, acc: 0.7932\n","lambda= 0.7 Epoch: [91/100], cls_loss: 0.7293, transfer_loss: 0.0961, total_Loss: 0.7965, acc: 0.7932\n","lambda= 0.7 Epoch: [92/100], cls_loss: 0.8217, transfer_loss: 0.0926, total_Loss: 0.8865, acc: 0.7851\n","lambda= 0.7 Epoch: [93/100], cls_loss: 0.7136, transfer_loss: 0.0949, total_Loss: 0.7800, acc: 0.7671\n","lambda= 0.7 Epoch: [94/100], cls_loss: 0.7373, transfer_loss: 0.0905, total_Loss: 0.8007, acc: 0.7631\n","lambda= 0.7 Epoch: [95/100], cls_loss: 0.7745, transfer_loss: 0.0902, total_Loss: 0.8376, acc: 0.7691\n","lambda= 0.7 Epoch: [96/100], cls_loss: 0.7158, transfer_loss: 0.0930, total_Loss: 0.7809, acc: 0.7771\n","lambda= 0.7 Epoch: [97/100], cls_loss: 0.6943, transfer_loss: 0.0900, total_Loss: 0.7573, acc: 0.7932\n","lambda= 0.7 Epoch: [98/100], cls_loss: 0.6283, transfer_loss: 0.0894, total_Loss: 0.6908, acc: 0.7952\n","lambda= 0.7 Epoch: [99/100], cls_loss: 0.6335, transfer_loss: 0.0884, total_Loss: 0.6954, acc: 0.7811\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.7000000000000001 Test accuracy: 0.8012048192771084\n","lambda= 0.8 Epoch: [ 0/100], cls_loss: 3.4300, transfer_loss: 0.1642, total_Loss: 3.5614, acc: 0.0462\n","lambda= 0.8 Epoch: [ 1/100], cls_loss: 3.4055, transfer_loss: 0.1642, total_Loss: 3.5369, acc: 0.0924\n","lambda= 0.8 Epoch: [ 2/100], cls_loss: 3.3836, transfer_loss: 0.1635, total_Loss: 3.5144, acc: 0.1165\n","lambda= 0.8 Epoch: [ 3/100], cls_loss: 3.3448, transfer_loss: 0.1640, total_Loss: 3.4760, acc: 0.1627\n","lambda= 0.8 Epoch: [ 4/100], cls_loss: 3.3333, transfer_loss: 0.1626, total_Loss: 3.4633, acc: 0.1606\n","lambda= 0.8 Epoch: [ 5/100], cls_loss: 3.2965, transfer_loss: 0.1651, total_Loss: 3.4286, acc: 0.2088\n","lambda= 0.8 Epoch: [ 6/100], cls_loss: 3.2812, transfer_loss: 0.1634, total_Loss: 3.4120, acc: 0.3193\n","lambda= 0.8 Epoch: [ 7/100], cls_loss: 3.2265, transfer_loss: 0.1646, total_Loss: 3.3582, acc: 0.4277\n","lambda= 0.8 Epoch: [ 8/100], cls_loss: 3.1656, transfer_loss: 0.1627, total_Loss: 3.2957, acc: 0.4880\n","lambda= 0.8 Epoch: [ 9/100], cls_loss: 3.1490, transfer_loss: 0.1633, total_Loss: 3.2797, acc: 0.4558\n","lambda= 0.8 Epoch: [10/100], cls_loss: 3.1274, transfer_loss: 0.1644, total_Loss: 3.2589, acc: 0.4639\n","lambda= 0.8 Epoch: [11/100], cls_loss: 3.0417, transfer_loss: 0.1646, total_Loss: 3.1733, acc: 0.5321\n","lambda= 0.8 Epoch: [12/100], cls_loss: 2.9987, transfer_loss: 0.1649, total_Loss: 3.1306, acc: 0.5582\n","lambda= 0.8 Epoch: [13/100], cls_loss: 2.8980, transfer_loss: 0.1618, total_Loss: 3.0274, acc: 0.5261\n","lambda= 0.8 Epoch: [14/100], cls_loss: 2.8487, transfer_loss: 0.1627, total_Loss: 2.9789, acc: 0.5522\n","lambda= 0.8 Epoch: [15/100], cls_loss: 2.7631, transfer_loss: 0.1663, total_Loss: 2.8961, acc: 0.4418\n","lambda= 0.8 Epoch: [16/100], cls_loss: 2.6612, transfer_loss: 0.1624, total_Loss: 2.7911, acc: 0.5321\n","lambda= 0.8 Epoch: [17/100], cls_loss: 2.6340, transfer_loss: 0.1633, total_Loss: 2.7646, acc: 0.5221\n","lambda= 0.8 Epoch: [18/100], cls_loss: 2.5363, transfer_loss: 0.1631, total_Loss: 2.6668, acc: 0.4779\n","lambda= 0.8 Epoch: [19/100], cls_loss: 2.4650, transfer_loss: 0.1633, total_Loss: 2.5957, acc: 0.5402\n","lambda= 0.8 Epoch: [20/100], cls_loss: 2.3823, transfer_loss: 0.1644, total_Loss: 2.5138, acc: 0.6185\n","lambda= 0.8 Epoch: [21/100], cls_loss: 2.2815, transfer_loss: 0.1646, total_Loss: 2.4132, acc: 0.6406\n","lambda= 0.8 Epoch: [22/100], cls_loss: 2.2255, transfer_loss: 0.1631, total_Loss: 2.3560, acc: 0.6365\n","lambda= 0.8 Epoch: [23/100], cls_loss: 2.0719, transfer_loss: 0.1625, total_Loss: 2.2018, acc: 0.6566\n","lambda= 0.8 Epoch: [24/100], cls_loss: 1.9690, transfer_loss: 0.1641, total_Loss: 2.1002, acc: 0.6586\n","lambda= 0.8 Epoch: [25/100], cls_loss: 1.8555, transfer_loss: 0.1629, total_Loss: 1.9858, acc: 0.6406\n","lambda= 0.8 Epoch: [26/100], cls_loss: 1.8346, transfer_loss: 0.1631, total_Loss: 1.9650, acc: 0.6767\n","lambda= 0.8 Epoch: [27/100], cls_loss: 1.7921, transfer_loss: 0.1621, total_Loss: 1.9218, acc: 0.6305\n","lambda= 0.8 Epoch: [28/100], cls_loss: 1.7189, transfer_loss: 0.1616, total_Loss: 1.8482, acc: 0.6606\n","lambda= 0.8 Epoch: [29/100], cls_loss: 1.6209, transfer_loss: 0.1639, total_Loss: 1.7520, acc: 0.6847\n","lambda= 0.8 Epoch: [30/100], cls_loss: 1.6163, transfer_loss: 0.1620, total_Loss: 1.7459, acc: 0.6667\n","lambda= 0.8 Epoch: [31/100], cls_loss: 1.5937, transfer_loss: 0.1625, total_Loss: 1.7237, acc: 0.6285\n","lambda= 0.8 Epoch: [32/100], cls_loss: 1.5082, transfer_loss: 0.1609, total_Loss: 1.6369, acc: 0.6847\n","lambda= 0.8 Epoch: [33/100], cls_loss: 1.4196, transfer_loss: 0.1608, total_Loss: 1.5482, acc: 0.6968\n","lambda= 0.8 Epoch: [34/100], cls_loss: 1.4399, transfer_loss: 0.1606, total_Loss: 1.5684, acc: 0.6687\n","lambda= 0.8 Epoch: [35/100], cls_loss: 1.3711, transfer_loss: 0.1609, total_Loss: 1.4998, acc: 0.6888\n","lambda= 0.8 Epoch: [36/100], cls_loss: 1.3324, transfer_loss: 0.1623, total_Loss: 1.4622, acc: 0.6707\n","lambda= 0.8 Epoch: [37/100], cls_loss: 1.2711, transfer_loss: 0.1623, total_Loss: 1.4009, acc: 0.6847\n","lambda= 0.8 Epoch: [38/100], cls_loss: 1.3056, transfer_loss: 0.1617, total_Loss: 1.4350, acc: 0.6968\n","lambda= 0.8 Epoch: [39/100], cls_loss: 1.2268, transfer_loss: 0.1591, total_Loss: 1.3541, acc: 0.7249\n","lambda= 0.8 Epoch: [40/100], cls_loss: 1.2567, transfer_loss: 0.1602, total_Loss: 1.3848, acc: 0.7108\n","lambda= 0.8 Epoch: [41/100], cls_loss: 1.1135, transfer_loss: 0.1597, total_Loss: 1.2413, acc: 0.6988\n","lambda= 0.8 Epoch: [42/100], cls_loss: 1.2963, transfer_loss: 0.1576, total_Loss: 1.4224, acc: 0.7269\n","lambda= 0.8 Epoch: [43/100], cls_loss: 1.1669, transfer_loss: 0.1577, total_Loss: 1.2931, acc: 0.7711\n","lambda= 0.8 Epoch: [44/100], cls_loss: 1.1080, transfer_loss: 0.1582, total_Loss: 1.2346, acc: 0.7470\n","lambda= 0.8 Epoch: [45/100], cls_loss: 1.1257, transfer_loss: 0.1569, total_Loss: 1.2512, acc: 0.7309\n","lambda= 0.8 Epoch: [46/100], cls_loss: 1.0199, transfer_loss: 0.1561, total_Loss: 1.1448, acc: 0.7169\n","lambda= 0.8 Epoch: [47/100], cls_loss: 1.1478, transfer_loss: 0.1520, total_Loss: 1.2694, acc: 0.7329\n","lambda= 0.8 Epoch: [48/100], cls_loss: 1.0483, transfer_loss: 0.1541, total_Loss: 1.1716, acc: 0.7149\n","lambda= 0.8 Epoch: [49/100], cls_loss: 1.0271, transfer_loss: 0.1529, total_Loss: 1.1494, acc: 0.7349\n","lambda= 0.8 Epoch: [50/100], cls_loss: 0.9654, transfer_loss: 0.1510, total_Loss: 1.0862, acc: 0.7369\n","lambda= 0.8 Epoch: [51/100], cls_loss: 0.9914, transfer_loss: 0.1463, total_Loss: 1.1084, acc: 0.7209\n","lambda= 0.8 Epoch: [52/100], cls_loss: 0.9569, transfer_loss: 0.1459, total_Loss: 1.0737, acc: 0.7410\n","lambda= 0.8 Epoch: [53/100], cls_loss: 1.0044, transfer_loss: 0.1472, total_Loss: 1.1222, acc: 0.7410\n","lambda= 0.8 Epoch: [54/100], cls_loss: 0.9701, transfer_loss: 0.1415, total_Loss: 1.0833, acc: 0.7450\n","lambda= 0.8 Epoch: [55/100], cls_loss: 1.0435, transfer_loss: 0.1425, total_Loss: 1.1575, acc: 0.7570\n","lambda= 0.8 Epoch: [56/100], cls_loss: 0.8801, transfer_loss: 0.1410, total_Loss: 0.9929, acc: 0.7450\n","lambda= 0.8 Epoch: [57/100], cls_loss: 0.9687, transfer_loss: 0.1372, total_Loss: 1.0784, acc: 0.7590\n","lambda= 0.8 Epoch: [58/100], cls_loss: 0.9297, transfer_loss: 0.1367, total_Loss: 1.0391, acc: 0.7631\n","lambda= 0.8 Epoch: [59/100], cls_loss: 0.9847, transfer_loss: 0.1372, total_Loss: 1.0945, acc: 0.7791\n","lambda= 0.8 Epoch: [60/100], cls_loss: 0.9723, transfer_loss: 0.1349, total_Loss: 1.0802, acc: 0.7932\n","lambda= 0.8 Epoch: [61/100], cls_loss: 0.9228, transfer_loss: 0.1327, total_Loss: 1.0290, acc: 0.7490\n","lambda= 0.8 Epoch: [62/100], cls_loss: 0.9271, transfer_loss: 0.1330, total_Loss: 1.0335, acc: 0.7510\n","lambda= 0.8 Epoch: [63/100], cls_loss: 0.8995, transfer_loss: 0.1307, total_Loss: 1.0041, acc: 0.7550\n","lambda= 0.8 Epoch: [64/100], cls_loss: 0.8512, transfer_loss: 0.1310, total_Loss: 0.9560, acc: 0.7751\n","lambda= 0.8 Epoch: [65/100], cls_loss: 0.8861, transfer_loss: 0.1292, total_Loss: 0.9894, acc: 0.7892\n","lambda= 0.8 Epoch: [66/100], cls_loss: 0.8139, transfer_loss: 0.1294, total_Loss: 0.9174, acc: 0.7570\n","lambda= 0.8 Epoch: [67/100], cls_loss: 0.8070, transfer_loss: 0.1256, total_Loss: 0.9075, acc: 0.7530\n","lambda= 0.8 Epoch: [68/100], cls_loss: 0.9184, transfer_loss: 0.1259, total_Loss: 1.0191, acc: 0.7390\n","lambda= 0.8 Epoch: [69/100], cls_loss: 0.8688, transfer_loss: 0.1239, total_Loss: 0.9679, acc: 0.7570\n","lambda= 0.8 Epoch: [70/100], cls_loss: 0.7596, transfer_loss: 0.1208, total_Loss: 0.8562, acc: 0.7530\n","lambda= 0.8 Epoch: [71/100], cls_loss: 0.8628, transfer_loss: 0.1226, total_Loss: 0.9609, acc: 0.7610\n","lambda= 0.8 Epoch: [72/100], cls_loss: 0.8400, transfer_loss: 0.1198, total_Loss: 0.9358, acc: 0.7851\n","lambda= 0.8 Epoch: [73/100], cls_loss: 0.8360, transfer_loss: 0.1196, total_Loss: 0.9317, acc: 0.7711\n","lambda= 0.8 Epoch: [74/100], cls_loss: 0.7902, transfer_loss: 0.1205, total_Loss: 0.8866, acc: 0.7631\n","lambda= 0.8 Epoch: [75/100], cls_loss: 0.7461, transfer_loss: 0.1211, total_Loss: 0.8430, acc: 0.7671\n","lambda= 0.8 Epoch: [76/100], cls_loss: 0.7865, transfer_loss: 0.1192, total_Loss: 0.8819, acc: 0.7691\n","lambda= 0.8 Epoch: [77/100], cls_loss: 0.7480, transfer_loss: 0.1183, total_Loss: 0.8427, acc: 0.7610\n","lambda= 0.8 Epoch: [78/100], cls_loss: 0.8330, transfer_loss: 0.1163, total_Loss: 0.9261, acc: 0.7831\n","lambda= 0.8 Epoch: [79/100], cls_loss: 0.7778, transfer_loss: 0.1173, total_Loss: 0.8716, acc: 0.7510\n","lambda= 0.8 Epoch: [80/100], cls_loss: 0.8311, transfer_loss: 0.1154, total_Loss: 0.9234, acc: 0.7550\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.8 Test accuracy: 0.7931726907630522\n","lambda= 0.9 Epoch: [ 0/100], cls_loss: 3.4393, transfer_loss: 0.1625, total_Loss: 3.5856, acc: 0.0562\n","lambda= 0.9 Epoch: [ 1/100], cls_loss: 3.4113, transfer_loss: 0.1646, total_Loss: 3.5595, acc: 0.0843\n","lambda= 0.9 Epoch: [ 2/100], cls_loss: 3.3885, transfer_loss: 0.1652, total_Loss: 3.5371, acc: 0.1546\n","lambda= 0.9 Epoch: [ 3/100], cls_loss: 3.3486, transfer_loss: 0.1630, total_Loss: 3.4953, acc: 0.2169\n","lambda= 0.9 Epoch: [ 4/100], cls_loss: 3.3281, transfer_loss: 0.1638, total_Loss: 3.4755, acc: 0.2169\n","lambda= 0.9 Epoch: [ 5/100], cls_loss: 3.3008, transfer_loss: 0.1626, total_Loss: 3.4472, acc: 0.3153\n","lambda= 0.9 Epoch: [ 6/100], cls_loss: 3.2784, transfer_loss: 0.1644, total_Loss: 3.4264, acc: 0.3434\n","lambda= 0.9 Epoch: [ 7/100], cls_loss: 3.2275, transfer_loss: 0.1630, total_Loss: 3.3741, acc: 0.3594\n","lambda= 0.9 Epoch: [ 8/100], cls_loss: 3.1629, transfer_loss: 0.1642, total_Loss: 3.3107, acc: 0.3353\n","lambda= 0.9 Epoch: [ 9/100], cls_loss: 3.1712, transfer_loss: 0.1642, total_Loss: 3.3189, acc: 0.3876\n","lambda= 0.9 Epoch: [10/100], cls_loss: 3.1069, transfer_loss: 0.1631, total_Loss: 3.2537, acc: 0.4880\n","lambda= 0.9 Epoch: [11/100], cls_loss: 3.0805, transfer_loss: 0.1632, total_Loss: 3.2274, acc: 0.5221\n","lambda= 0.9 Epoch: [12/100], cls_loss: 2.9904, transfer_loss: 0.1622, total_Loss: 3.1363, acc: 0.5120\n","lambda= 0.9 Epoch: [13/100], cls_loss: 2.9476, transfer_loss: 0.1628, total_Loss: 3.0941, acc: 0.5181\n","lambda= 0.9 Epoch: [14/100], cls_loss: 2.8752, transfer_loss: 0.1637, total_Loss: 3.0226, acc: 0.5843\n","lambda= 0.9 Epoch: [15/100], cls_loss: 2.7753, transfer_loss: 0.1613, total_Loss: 2.9205, acc: 0.5562\n","lambda= 0.9 Epoch: [16/100], cls_loss: 2.7061, transfer_loss: 0.1611, total_Loss: 2.8511, acc: 0.5261\n","lambda= 0.9 Epoch: [17/100], cls_loss: 2.6040, transfer_loss: 0.1626, total_Loss: 2.7503, acc: 0.5663\n","lambda= 0.9 Epoch: [18/100], cls_loss: 2.5342, transfer_loss: 0.1622, total_Loss: 2.6802, acc: 0.5301\n","lambda= 0.9 Epoch: [19/100], cls_loss: 2.4459, transfer_loss: 0.1625, total_Loss: 2.5922, acc: 0.5763\n","lambda= 0.9 Epoch: [20/100], cls_loss: 2.3368, transfer_loss: 0.1624, total_Loss: 2.4830, acc: 0.6165\n","lambda= 0.9 Epoch: [21/100], cls_loss: 2.3311, transfer_loss: 0.1634, total_Loss: 2.4781, acc: 0.5843\n","lambda= 0.9 Epoch: [22/100], cls_loss: 2.2092, transfer_loss: 0.1631, total_Loss: 2.3560, acc: 0.6386\n","lambda= 0.9 Epoch: [23/100], cls_loss: 2.1164, transfer_loss: 0.1628, total_Loss: 2.2629, acc: 0.6225\n","lambda= 0.9 Epoch: [24/100], cls_loss: 2.1048, transfer_loss: 0.1644, total_Loss: 2.2528, acc: 0.6386\n","lambda= 0.9 Epoch: [25/100], cls_loss: 1.9240, transfer_loss: 0.1628, total_Loss: 2.0705, acc: 0.6185\n","lambda= 0.9 Epoch: [26/100], cls_loss: 1.9036, transfer_loss: 0.1636, total_Loss: 2.0508, acc: 0.6245\n","lambda= 0.9 Epoch: [27/100], cls_loss: 1.7964, transfer_loss: 0.1626, total_Loss: 1.9427, acc: 0.6888\n","lambda= 0.9 Epoch: [28/100], cls_loss: 1.7026, transfer_loss: 0.1615, total_Loss: 1.8480, acc: 0.6647\n","lambda= 0.9 Epoch: [29/100], cls_loss: 1.6622, transfer_loss: 0.1622, total_Loss: 1.8082, acc: 0.6546\n","lambda= 0.9 Epoch: [30/100], cls_loss: 1.6724, transfer_loss: 0.1604, total_Loss: 1.8168, acc: 0.6928\n","lambda= 0.9 Epoch: [31/100], cls_loss: 1.5286, transfer_loss: 0.1621, total_Loss: 1.6744, acc: 0.7068\n","lambda= 0.9 Epoch: [32/100], cls_loss: 1.5304, transfer_loss: 0.1611, total_Loss: 1.6754, acc: 0.6847\n","lambda= 0.9 Epoch: [33/100], cls_loss: 1.4369, transfer_loss: 0.1603, total_Loss: 1.5812, acc: 0.6687\n","lambda= 0.9 Epoch: [34/100], cls_loss: 1.4336, transfer_loss: 0.1583, total_Loss: 1.5761, acc: 0.7088\n","lambda= 0.9 Epoch: [35/100], cls_loss: 1.3022, transfer_loss: 0.1588, total_Loss: 1.4451, acc: 0.7028\n","lambda= 0.9 Epoch: [36/100], cls_loss: 1.2891, transfer_loss: 0.1573, total_Loss: 1.4307, acc: 0.7088\n","lambda= 0.9 Epoch: [37/100], cls_loss: 1.2215, transfer_loss: 0.1561, total_Loss: 1.3620, acc: 0.6928\n","lambda= 0.9 Epoch: [38/100], cls_loss: 1.3324, transfer_loss: 0.1572, total_Loss: 1.4739, acc: 0.7108\n","lambda= 0.9 Epoch: [39/100], cls_loss: 1.2095, transfer_loss: 0.1549, total_Loss: 1.3489, acc: 0.6968\n","lambda= 0.9 Epoch: [40/100], cls_loss: 1.1411, transfer_loss: 0.1530, total_Loss: 1.2788, acc: 0.6687\n","lambda= 0.9 Epoch: [41/100], cls_loss: 1.1797, transfer_loss: 0.1511, total_Loss: 1.3157, acc: 0.6847\n","lambda= 0.9 Epoch: [42/100], cls_loss: 1.1375, transfer_loss: 0.1522, total_Loss: 1.2745, acc: 0.7108\n","lambda= 0.9 Epoch: [43/100], cls_loss: 1.1218, transfer_loss: 0.1465, total_Loss: 1.2536, acc: 0.7108\n","lambda= 0.9 Epoch: [44/100], cls_loss: 1.1538, transfer_loss: 0.1461, total_Loss: 1.2854, acc: 0.7129\n","lambda= 0.9 Epoch: [45/100], cls_loss: 1.1151, transfer_loss: 0.1447, total_Loss: 1.2453, acc: 0.7430\n","lambda= 0.9 Epoch: [46/100], cls_loss: 1.0728, transfer_loss: 0.1396, total_Loss: 1.1985, acc: 0.7048\n","lambda= 0.9 Epoch: [47/100], cls_loss: 1.1039, transfer_loss: 0.1379, total_Loss: 1.2280, acc: 0.7149\n","lambda= 0.9 Epoch: [48/100], cls_loss: 1.0797, transfer_loss: 0.1350, total_Loss: 1.2012, acc: 0.7269\n","lambda= 0.9 Epoch: [49/100], cls_loss: 1.0392, transfer_loss: 0.1333, total_Loss: 1.1591, acc: 0.7269\n","lambda= 0.9 Epoch: [50/100], cls_loss: 1.0714, transfer_loss: 0.1334, total_Loss: 1.1914, acc: 0.7470\n","lambda= 0.9 Epoch: [51/100], cls_loss: 0.8963, transfer_loss: 0.1307, total_Loss: 1.0139, acc: 0.7631\n","lambda= 0.9 Epoch: [52/100], cls_loss: 1.0644, transfer_loss: 0.1269, total_Loss: 1.1786, acc: 0.7510\n","lambda= 0.9 Epoch: [53/100], cls_loss: 1.0160, transfer_loss: 0.1265, total_Loss: 1.1299, acc: 0.7410\n","lambda= 0.9 Epoch: [54/100], cls_loss: 1.0426, transfer_loss: 0.1274, total_Loss: 1.1573, acc: 0.7349\n","lambda= 0.9 Epoch: [55/100], cls_loss: 1.0104, transfer_loss: 0.1216, total_Loss: 1.1198, acc: 0.7309\n","lambda= 0.9 Epoch: [56/100], cls_loss: 0.9250, transfer_loss: 0.1222, total_Loss: 1.0350, acc: 0.7430\n","lambda= 0.9 Epoch: [57/100], cls_loss: 1.0093, transfer_loss: 0.1191, total_Loss: 1.1165, acc: 0.7309\n","lambda= 0.9 Epoch: [58/100], cls_loss: 0.9074, transfer_loss: 0.1171, total_Loss: 1.0128, acc: 0.7410\n","lambda= 0.9 Epoch: [59/100], cls_loss: 0.9278, transfer_loss: 0.1173, total_Loss: 1.0334, acc: 0.7149\n","lambda= 0.9 Epoch: [60/100], cls_loss: 0.9385, transfer_loss: 0.1181, total_Loss: 1.0448, acc: 0.7309\n","lambda= 0.9 Epoch: [61/100], cls_loss: 0.8391, transfer_loss: 0.1165, total_Loss: 0.9439, acc: 0.7390\n","lambda= 0.9 Epoch: [62/100], cls_loss: 0.8858, transfer_loss: 0.1128, total_Loss: 0.9873, acc: 0.7430\n","lambda= 0.9 Epoch: [63/100], cls_loss: 0.9849, transfer_loss: 0.1154, total_Loss: 1.0887, acc: 0.7590\n","lambda= 0.9 Epoch: [64/100], cls_loss: 0.9728, transfer_loss: 0.1132, total_Loss: 1.0747, acc: 0.7510\n","lambda= 0.9 Epoch: [65/100], cls_loss: 0.9771, transfer_loss: 0.1109, total_Loss: 1.0769, acc: 0.7631\n","lambda= 0.9 Epoch: [66/100], cls_loss: 0.9012, transfer_loss: 0.1091, total_Loss: 0.9994, acc: 0.7671\n","lambda= 0.9 Epoch: [67/100], cls_loss: 0.9332, transfer_loss: 0.1067, total_Loss: 1.0292, acc: 0.7651\n","lambda= 0.9 Epoch: [68/100], cls_loss: 0.9103, transfer_loss: 0.1081, total_Loss: 1.0076, acc: 0.7631\n","lambda= 0.9 Epoch: [69/100], cls_loss: 0.8375, transfer_loss: 0.1113, total_Loss: 0.9377, acc: 0.7510\n","lambda= 0.9 Epoch: [70/100], cls_loss: 0.8618, transfer_loss: 0.1064, total_Loss: 0.9576, acc: 0.7711\n","lambda= 0.9 Epoch: [71/100], cls_loss: 0.8327, transfer_loss: 0.1046, total_Loss: 0.9269, acc: 0.7731\n","lambda= 0.9 Epoch: [72/100], cls_loss: 0.8458, transfer_loss: 0.1047, total_Loss: 0.9401, acc: 0.7490\n","lambda= 0.9 Epoch: [73/100], cls_loss: 0.8273, transfer_loss: 0.1053, total_Loss: 0.9221, acc: 0.7731\n","lambda= 0.9 Epoch: [74/100], cls_loss: 0.7592, transfer_loss: 0.1040, total_Loss: 0.8528, acc: 0.7570\n","lambda= 0.9 Epoch: [75/100], cls_loss: 0.7375, transfer_loss: 0.1034, total_Loss: 0.8305, acc: 0.7490\n","lambda= 0.9 Epoch: [76/100], cls_loss: 0.6975, transfer_loss: 0.1039, total_Loss: 0.7910, acc: 0.7490\n","lambda= 0.9 Epoch: [77/100], cls_loss: 0.7639, transfer_loss: 0.1019, total_Loss: 0.8556, acc: 0.7570\n","lambda= 0.9 Epoch: [78/100], cls_loss: 0.8036, transfer_loss: 0.1026, total_Loss: 0.8959, acc: 0.7510\n","lambda= 0.9 Epoch: [79/100], cls_loss: 0.6738, transfer_loss: 0.1001, total_Loss: 0.7639, acc: 0.7490\n","lambda= 0.9 Epoch: [80/100], cls_loss: 0.7650, transfer_loss: 0.0992, total_Loss: 0.8543, acc: 0.7470\n","lambda= 0.9 Epoch: [81/100], cls_loss: 0.8077, transfer_loss: 0.1020, total_Loss: 0.8995, acc: 0.7671\n","lambda= 0.9 Epoch: [82/100], cls_loss: 0.8075, transfer_loss: 0.0981, total_Loss: 0.8958, acc: 0.7831\n","lambda= 0.9 Epoch: [83/100], cls_loss: 0.7063, transfer_loss: 0.1000, total_Loss: 0.7963, acc: 0.7731\n","lambda= 0.9 Epoch: [84/100], cls_loss: 0.6650, transfer_loss: 0.0986, total_Loss: 0.7537, acc: 0.7550\n","lambda= 0.9 Epoch: [85/100], cls_loss: 0.6558, transfer_loss: 0.0983, total_Loss: 0.7443, acc: 0.7530\n","lambda= 0.9 Epoch: [86/100], cls_loss: 0.7327, transfer_loss: 0.0974, total_Loss: 0.8203, acc: 0.7570\n","lambda= 0.9 Epoch: [87/100], cls_loss: 0.7794, transfer_loss: 0.0950, total_Loss: 0.8648, acc: 0.7470\n","lambda= 0.9 Epoch: [88/100], cls_loss: 0.7032, transfer_loss: 0.0998, total_Loss: 0.7930, acc: 0.7691\n","lambda= 0.9 Epoch: [89/100], cls_loss: 0.7661, transfer_loss: 0.0933, total_Loss: 0.8501, acc: 0.7952\n","lambda= 0.9 Epoch: [90/100], cls_loss: 0.7786, transfer_loss: 0.0959, total_Loss: 0.8649, acc: 0.7751\n","lambda= 0.9 Epoch: [91/100], cls_loss: 0.6594, transfer_loss: 0.0936, total_Loss: 0.7437, acc: 0.7831\n","lambda= 0.9 Epoch: [92/100], cls_loss: 0.7193, transfer_loss: 0.0948, total_Loss: 0.8046, acc: 0.7851\n","lambda= 0.9 Epoch: [93/100], cls_loss: 0.6843, transfer_loss: 0.0982, total_Loss: 0.7727, acc: 0.7530\n","lambda= 0.9 Epoch: [94/100], cls_loss: 0.7066, transfer_loss: 0.0957, total_Loss: 0.7927, acc: 0.7711\n","lambda= 0.9 Epoch: [95/100], cls_loss: 0.6777, transfer_loss: 0.0945, total_Loss: 0.7627, acc: 0.7711\n","lambda= 0.9 Epoch: [96/100], cls_loss: 0.6224, transfer_loss: 0.0918, total_Loss: 0.7050, acc: 0.7851\n","lambda= 0.9 Epoch: [97/100], cls_loss: 0.7426, transfer_loss: 0.0890, total_Loss: 0.8227, acc: 0.7871\n","lambda= 0.9 Epoch: [98/100], cls_loss: 0.6244, transfer_loss: 0.0922, total_Loss: 0.7074, acc: 0.7771\n","lambda= 0.9 Epoch: [99/100], cls_loss: 0.6952, transfer_loss: 0.0910, total_Loss: 0.7771, acc: 0.7671\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-a949cdfed337>:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["lambda= 0.9 Test accuracy: 0.7951807228915662\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-21-47478dc6ccbe>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  test_result = test_result.append(pd.DataFrame(records), ignore_index=True)\n"]}],"source":["import numpy as np\n","test_result = pd.DataFrame(columns=[\"lambda\", \"Test accuracy\"])\n","train_result = pd.DataFrame()\n","test_result = pd.DataFrame()\n","\n","import gc\n","train_result = pd.DataFrame() # new added\n","test_result = pd.DataFrame()\n","for lamb in np.arange(0, 1, 0.10):\n","    transfer_loss = 'mmd'\n","    learning_rate = 0.0001\n","    transfer_model = TransferNet(n_class, transfer_loss=transfer_loss, base_net='resnet101').cuda()\n","    optimizer = torch.optim.SGD([\n","        {'params': transfer_model.base_network.parameters()},\n","        {'params': transfer_model.bottleneck_layer.parameters(), 'lr': 10 * learning_rate},\n","        {'params': transfer_model.classifier_layer.parameters(), 'lr': 10 * learning_rate},\n","    ], lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n","    # lamb = 0.5 # weight for transfer loss, it is a hyperparameter that needs to be tuned\n","\n","    # train_result = pd.DataFrame() # new added\n","    # test_result = pd.DataFrame()\n","    def train2(dataloaders, model, optimizer, lamb):\n","        source_loader, target_train_loader, target_test_loader = dataloaders['src'], dataloaders['val'], dataloaders['tar']\n","        len_source_loader = len(source_loader)\n","        len_target_loader = len(target_train_loader)\n","        best_acc = 0\n","        stop = 0\n","        n_batch = min(len_source_loader, len_target_loader)\n","        records = [] # new added\n","        for e in range(n_epoch):\n","            stop += 1\n","            train_loss_clf, train_loss_transfer, train_loss_total = 0, 0, 0\n","            model.train()\n","            for (src, tar) in zip(source_loader, target_train_loader):\n","                data_source, label_source = src\n","                data_target, _ = tar\n","                data_source, label_source = data_source.cuda(), label_source.cuda()\n","                data_target = data_target.cuda()\n","\n","                optimizer.zero_grad()\n","                label_source_pred, transfer_loss = model(data_source, data_target)\n","                clf_loss = criterion(label_source_pred, label_source)\n","                loss = clf_loss + lamb * transfer_loss\n","                loss.backward()\n","                optimizer.step()\n","                train_loss_clf = clf_loss.detach().item() + train_loss_clf\n","                train_loss_transfer = transfer_loss.detach().item() + train_loss_transfer\n","                train_loss_total = loss.detach().item() + train_loss_total\n","            acc = test(model, target_test_loader)\n","            print(f'lambda= {lamb}',f'Epoch: [{e:2d}/{n_epoch}], cls_loss: {train_loss_clf/n_batch:.4f}, transfer_loss: {train_loss_transfer/n_batch:.4f}, total_Loss: {train_loss_total/n_batch:.4f}, acc: {acc:.4f}')\n","            records.append({ # new added\n","                'Lambda': lamb,\n","                'Epoch': e,\n","                'Cls_Loss': train_loss_clf/n_batch,\n","                'Transfer_Loss': train_loss_transfer/n_batch,\n","                'Total_Loss': train_loss_total/n_batch,\n","                'Accuracy': acc\n","            })\n","\n","            if best_acc < acc:\n","                best_acc = acc\n","                torch.save(model.state_dict(), 'trans_model_webcam.pkl')\n","                stop = 0\n","            if stop >= early_stop:\n","                break\n","\n","        global train_result\n","        train_result = train_result.append(pd.DataFrame(records), ignore_index=True)\n","        # train_result.to_csv(os.path.join(data_folder, f'train_result_amazon_webcam_resnet101_lambda {lamb}.csv'), index=False)\n","\n","    train2(dataloaders, transfer_model, optimizer, lamb.round(1))\n","    transfer_model.load_state_dict(torch.load('trans_model_webcam.pkl'))\n","    acc_test = test2(transfer_model, dataloaders['tar'], lamb)\n","    print(f'lambda= {lamb}', f'Test accuracy: {acc_test}')\n","    del transfer_model\n","    # del train2\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":939,"status":"ok","timestamp":1698810745055,"user":{"displayName":"Iman Khazrak","userId":"00007021905658904546"},"user_tz":240},"id":"soWHnJ31H17u"},"outputs":[],"source":["train_result.to_csv(os.path.join(data_folder, 'train_result_amazon_dslr_resnet101.csv'), index=False)\n","test_result.to_csv(os.path.join(data_folder, 'test_result_amazon_dslr_resnet101.csv'), index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}